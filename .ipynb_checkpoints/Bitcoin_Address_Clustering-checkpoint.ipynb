{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sKhQWqDpkDA_"
   },
   "source": [
    "# **1. Global Constants**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Ky0KfLv3icc8"
   },
   "outputs": [],
   "source": [
    "GDRIVE_DIR = \"/content/gdrive\"\n",
    "GDRIVE_HOME_DIR = GDRIVE_DIR + \"/My Drive\"\n",
    "GDRIVE_DATA_DIR = GDRIVE_HOME_DIR + \"/Bitcoin Address Clustering/dataset\"\n",
    "\n",
    "start_block = 0\n",
    "end_block = 120000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qczfcwn5-xZm"
   },
   "source": [
    "# **2. Dependences**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tadn-hgKdLnb",
    "outputId": "b6a1a26b-fe04-4b35-959a-ae5b61123064"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting pyspark\n",
      "  Downloading pyspark-3.3.0.tar.gz (281.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 281.3 MB 36 kB/s \n",
      "\u001b[?25hCollecting py4j==0.10.9.5\n",
      "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
      "\u001b[K     |████████████████████████████████| 199 kB 49.1 MB/s \n",
      "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pyspark: filename=pyspark-3.3.0-py2.py3-none-any.whl size=281764026 sha256=308d9be186f657de2e4e6572f7a6c7ec97785dbd610851d48394d8d356ddc5a0\n",
      "  Stored in directory: /root/.cache/pip/wheels/7a/8e/1b/f73a52650d2e5f337708d9f6a1750d451a7349a867f928b885\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.5 pyspark-3.3.0\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting wget\n",
      "  Downloading wget-3.2.zip (10 kB)\n",
      "Building wheels for collected packages: wget\n",
      "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9675 sha256=a310455183136b323810dc58a141d8a5ddbeea0748435f1e9fd1d335241dad1b\n",
      "  Stored in directory: /root/.cache/pip/wheels/a1/b6/7c/0e63e34eb06634181c63adacca38b79ff8f35c37e3c13e3c02\n",
      "Successfully built wget\n",
      "Installing collected packages: wget\n",
      "Successfully installed wget-3.2\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting pyvis\n",
      "  Downloading pyvis-0.2.1.tar.gz (21 kB)\n",
      "Requirement already satisfied: jinja2>=2.9.6 in /usr/local/lib/python3.7/dist-packages (from pyvis) (2.11.3)\n",
      "Requirement already satisfied: networkx>=1.11 in /usr/local/lib/python3.7/dist-packages (from pyvis) (2.6.3)\n",
      "Requirement already satisfied: ipython>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from pyvis) (5.5.0)\n",
      "Collecting jsonpickle>=1.4.1\n",
      "  Downloading jsonpickle-2.2.0-py2.py3-none-any.whl (39 kB)\n",
      "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython>=5.3.0->pyvis) (0.8.1)\n",
      "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython>=5.3.0->pyvis) (57.4.0)\n",
      "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython>=5.3.0->pyvis) (4.8.0)\n",
      "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=5.3.0->pyvis) (2.6.1)\n",
      "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython>=5.3.0->pyvis) (1.0.18)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=5.3.0->pyvis) (4.4.2)\n",
      "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=5.3.0->pyvis) (0.7.5)\n",
      "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython>=5.3.0->pyvis) (5.1.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2>=2.9.6->pyvis) (2.0.1)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from jsonpickle>=1.4.1->pyvis) (4.11.4)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=5.3.0->pyvis) (0.2.5)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=5.3.0->pyvis) (1.15.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->jsonpickle>=1.4.1->pyvis) (4.1.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->jsonpickle>=1.4.1->pyvis) (3.8.0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->ipython>=5.3.0->pyvis) (0.7.0)\n",
      "Building wheels for collected packages: pyvis\n",
      "  Building wheel for pyvis (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pyvis: filename=pyvis-0.2.1-py3-none-any.whl size=23688 sha256=f96b0383f33553754855858256bc42807cd96eefc769eb2d0eb5040fe5ff9ee3\n",
      "  Stored in directory: /root/.cache/pip/wheels/2a/8f/04/6340d46afc74f59cc857a594ca1a2a14a1f4cbd4fd6c2e9306\n",
      "Successfully built pyvis\n",
      "Installing collected packages: jsonpickle, pyvis\n",
      "Successfully installed jsonpickle-2.2.0 pyvis-0.2.1\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark\n",
    "!pip install -U -q PyDrive\n",
    "!pip install wget\n",
    "!pip install pyvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "2vrGsesPQ6eZ"
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from google.colab import drive\n",
    "import requests\n",
    "import pprint\n",
    "import codecs\n",
    "import builtins\n",
    "import wget\n",
    "import csv\n",
    "import os\n",
    "from urllib import request\n",
    "from urllib.error import HTTPError\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "from pyvis.network import Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nPrycaK1kL9o"
   },
   "source": [
    "# **3. Spark and Google Colab Setup**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j9mB8waGlabj"
   },
   "source": [
    "## **3.1.** Create Spark context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "9IkyllulSwSr"
   },
   "outputs": [],
   "source": [
    "# create the session\n",
    "conf = SparkConf()\\\n",
    "                .set('spark.executor.memory', '50G')\\\n",
    "                .set('spark.driver.memory', '50G')\\\n",
    "                .set('spark.driver.maxResultSize', '50G')\\\n",
    "                .set(\"spark.driver.cores\", \"10\")\\\n",
    "                .set(\"spark.sql.analyzer.maxIterations\", \"100000\")\n",
    "\n",
    "# create the context\n",
    "sc = SparkContext(conf=conf)\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IOZd-zdGlgfZ"
   },
   "source": [
    "## **3.2.** Link Colab to our Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vIcprUr4Q8_8",
    "outputId": "409a81d2-2fea-4056-b3a2-c2183fd36156"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "# Point Colaboratory to our Google Drive\n",
    "drive.mount(GDRIVE_DIR, force_remount=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CdPCjEcYlwX-"
   },
   "source": [
    "## **3.3.** Check everything is ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "xHLDfd-2lzQ8",
    "outputId": "af1364d0-edf0-460b-9f58-27a2eb57d6fa"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://031ec2b902d7:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f7fc05cfd10>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7yljSDtZl15h",
    "outputId": "0fb7d034-baca-427a-ac2d-6574cc3ba92d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.driver.extraJavaOptions',\n",
       "  '-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED'),\n",
       " ('spark.executor.memory', '50G'),\n",
       " ('spark.driver.cores', '10'),\n",
       " ('spark.sql.analyzer.maxIterations', '100000'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.sql.warehouse.dir', 'file:/content/spark-warehouse'),\n",
       " ('spark.driver.memory', '50G'),\n",
       " ('spark.app.name', 'pyspark-shell'),\n",
       " ('spark.app.submitTime', '1656284133570'),\n",
       " ('spark.driver.port', '43443'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.executor.extraJavaOptions',\n",
       "  '-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.app.id', 'local-1656284138451'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.driver.host', '031ec2b902d7'),\n",
       " ('spark.app.startTime', '1656284134032'),\n",
       " ('spark.driver.maxResultSize', '50G')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc._conf.getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yr_5pxwDl89m"
   },
   "source": [
    "# **4. Bitcoin Address Clustering**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M47ZrRvHMy7N"
   },
   "source": [
    "The main goal of the project is to obtain information from the Bitcoin blockchain. Before explaining in detail the project I want to develop, it is useful to mention the sources that inspired me:\n",
    "\n",
    "- The articles collected in the [shared link](https://drive.google.com/drive/folders/1rIEtP5jx-B0x4QzOsZJnwFVAuSuOfKEJ?usp=sharing)\n",
    "- The [OXT](https://oxt.me/) site and its features. especially the graphical visualization of the blockchain through the transaction graph\n",
    "\n",
    "Specifically, the project wants to manipulate the information extrapolated from the Bitcoin blockchain to create the transaction graph and use it to perform chain analysis. Bitcoin address clustering based on multiple heuristic conditions is the core of chain analysis.\n",
    "\n",
    "The project can be summarized in the following points:\n",
    "- construction of the dataset created starting from the Bitcoin blockchain\n",
    "- creation of the transaction graph starting from the dataset. The nodes of the graph are the transactions, the input edges are the input UTXOs to the transaction, and the output edges are the transaction output UTXOs.\n",
    "- implementation of heuristics:\n",
    "    - common-input-ownership heuristic\n",
    "    - change address detection heuristic\n",
    "    - Coinbase transaction mining address clustering heuristic\n",
    "    - multiple mining pool address clustering heuristic\n",
    "    - mixed transaction recognition heuristic\n",
    "    - Louvain community detection algorithm\n",
    "- perform graph analysis and use the heuristics created to analyze addresses, transactions and entities known thanks to off-chain analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hCZ6G78Qozu5"
   },
   "source": [
    "## **4.1.** Bitcoin Block Overview\n",
    "\n",
    "In this section, we see what is inside a bitcoin block and we recognize the useful informations to be extrapolated for our purpose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0w1ZEG31qAWz"
   },
   "source": [
    "### **4.1.1.** Make a request to [Blockchain.info](https://blockchain.info) to get a raw bitcoin block\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZzxI1cyth9Fr",
    "outputId": "c0f2f7eb-9765-47f7-98d4-9d89135395e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'blocks': [{'bits': 486604799,\n",
      "             'block_index': 0,\n",
      "             'fee': 0,\n",
      "             'hash': '000000000019d6689c085ae165831e934ff763ae46a2a6c172b3f1b60a8ce26f',\n",
      "             'height': 0,\n",
      "             'main_chain': True,\n",
      "             'mrkl_root': '4a5e1e4baab89f3a32518a88c31bc87f618f76673e2cc77ab2127b7afdeda33b',\n",
      "             'n_tx': 1,\n",
      "             'next_block': ['00000000839a8e6886ab5951d76f411475428afc90947ee320161bbf18eb6048'],\n",
      "             'nonce': 2083236893,\n",
      "             'prev_block': '0000000000000000000000000000000000000000000000000000000000000000',\n",
      "             'size': 285,\n",
      "             'time': 1231006505,\n",
      "             'tx': [{'block_height': 0,\n",
      "                     'block_index': 0,\n",
      "                     'double_spend': False,\n",
      "                     'fee': 0,\n",
      "                     'hash': '4a5e1e4baab89f3a32518a88c31bc87f618f76673e2cc77ab2127b7afdeda33b',\n",
      "                     'inputs': [{'index': 0,\n",
      "                                 'prev_out': {'n': 4294967295,\n",
      "                                              'script': '',\n",
      "                                              'spending_outpoints': [{'n': 0,\n",
      "                                                                      'tx_index': 2098408272645986}],\n",
      "                                              'spent': True,\n",
      "                                              'tx_index': 0,\n",
      "                                              'type': 0,\n",
      "                                              'value': 0},\n",
      "                                 'script': '04ffff001d0104455468652054696d65732030332f4a616e2f32303039204368616e63656c6c6f72206f6e206272696e6b206f66207365636f6e64206261696c6f757420666f722062616e6b73',\n",
      "                                 'sequence': 4294967295,\n",
      "                                 'witness': ''}],\n",
      "                     'lock_time': 0,\n",
      "                     'out': [{'addr': '1A1zP1eP5QGefi2DMPTfTL5SLmv7DivfNa',\n",
      "                              'n': 0,\n",
      "                              'script': '4104678afdb0fe5548271967f1a67130b7105cd6a828e03909a67962e0ea1f61deb649f6bc3f4cef38c4f35504e51ec112de5c384df7ba0b8d578a4c702b6bf11d5fac',\n",
      "                              'spending_outpoints': [],\n",
      "                              'spent': False,\n",
      "                              'tx_index': 2098408272645986,\n",
      "                              'type': 0,\n",
      "                              'value': 5000000000}],\n",
      "                     'relayed_by': '0.0.0.0',\n",
      "                     'size': 204,\n",
      "                     'time': 1231006505,\n",
      "                     'tx_index': 2098408272645986,\n",
      "                     'ver': 1,\n",
      "                     'vin_sz': 1,\n",
      "                     'vout_sz': 1,\n",
      "                     'weight': 816}],\n",
      "             'ver': 1,\n",
      "             'weight': 1140}]}\n"
     ]
    }
   ],
   "source": [
    "block_height = 0\n",
    "response = requests.get('https://blockchain.info/block-height/{}'.format(block_height))\n",
    "pprint.PrettyPrinter().pprint(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mzO8ieRpqt7J"
   },
   "source": [
    "### **4.1.2.** The Times 03/Jan/2009 Chancellor on brink of second bailout for banks\n",
    "Translating the hexadecimal script present in the first transaction of the first block of bitcoin into ASCII, we can read the message from Satoshi Nakamoto\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qAXFktEHW_z_",
    "outputId": "2ed0ca96-911b-412d-c284-ba47b24b4b67"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'\\x04\\xff\\xff\\x00\\x1d\\x01\\x04EThe Times 03/Jan/2009 Chancellor on brink of second bailout for banks'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string = response.json()['blocks'][0]['tx'][0]['inputs'][0]['script']\n",
    "codecs.decode(string, \"hex\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vSf2lVpJuKI5"
   },
   "source": [
    "### **4.1.3.** Block's usefull information\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e_Gu8Txouq1S",
    "outputId": "6f89b6b7-ca25-4c3a-e981-390731581639"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "block_height: 0\n",
      "block_hash: 000000000019d6689c085ae165831e934ff763ae46a2a6c172b3f1b60a8ce26f\n",
      "TXs_list:\n",
      "[{'block_height': 0,\n",
      "  'block_index': 0,\n",
      "  'double_spend': False,\n",
      "  'fee': 0,\n",
      "  'hash': '4a5e1e4baab89f3a32518a88c31bc87f618f76673e2cc77ab2127b7afdeda33b',\n",
      "  'inputs': [{'index': 0,\n",
      "              'prev_out': {'n': 4294967295,\n",
      "                           'script': '',\n",
      "                           'spending_outpoints': [{'n': 0,\n",
      "                                                   'tx_index': 2098408272645986}],\n",
      "                           'spent': True,\n",
      "                           'tx_index': 0,\n",
      "                           'type': 0,\n",
      "                           'value': 0},\n",
      "              'script': '04ffff001d0104455468652054696d65732030332f4a616e2f32303039204368616e63656c6c6f72206f6e206272696e6b206f66207365636f6e64206261696c6f757420666f722062616e6b73',\n",
      "              'sequence': 4294967295,\n",
      "              'witness': ''}],\n",
      "  'lock_time': 0,\n",
      "  'out': [{'addr': '1A1zP1eP5QGefi2DMPTfTL5SLmv7DivfNa',\n",
      "           'n': 0,\n",
      "           'script': '4104678afdb0fe5548271967f1a67130b7105cd6a828e03909a67962e0ea1f61deb649f6bc3f4cef38c4f35504e51ec112de5c384df7ba0b8d578a4c702b6bf11d5fac',\n",
      "           'spending_outpoints': [],\n",
      "           'spent': False,\n",
      "           'tx_index': 2098408272645986,\n",
      "           'type': 0,\n",
      "           'value': 5000000000}],\n",
      "  'relayed_by': '0.0.0.0',\n",
      "  'size': 204,\n",
      "  'time': 1231006505,\n",
      "  'tx_index': 2098408272645986,\n",
      "  'ver': 1,\n",
      "  'vin_sz': 1,\n",
      "  'vout_sz': 1,\n",
      "  'weight': 816}]\n"
     ]
    }
   ],
   "source": [
    "block = response.json()['blocks'][0]\n",
    "block_height = block['height']\n",
    "block_hash = block['hash']\n",
    "TXs_list = block['tx']\n",
    "print('block_height:', block_height)\n",
    "print('block_hash:', block_hash)\n",
    "print('TXs_list:')\n",
    "pprint.PrettyPrinter().pprint(TXs_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_CCaIhlWvkiv"
   },
   "source": [
    "### **4.1.4.** Transaction's usefull information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H1n-J7hyv0tI",
    "outputId": "c30a2d60-7b77-43e2-e3e3-0d376ff5592d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tx_id: 2098408272645986\n",
      "tx_hash: 4a5e1e4baab89f3a32518a88c31bc87f618f76673e2cc77ab2127b7afdeda33b\n",
      "fee: 0\n",
      "n_input: 1\n",
      "amount_input: 0\n",
      "n_output: 1\n",
      "amount_output: 5000000000\n"
     ]
    }
   ],
   "source": [
    "TX = TXs_list[0]\n",
    "tx_id = TX['tx_index']\n",
    "tx_hash = TX['hash']\n",
    "fee = TX['fee']\n",
    "n_input = len(TX['inputs'])\n",
    "amount_input = builtins.sum([UTXO['prev_out']['value'] for UTXO in TX['inputs']])\n",
    "n_output = len(TX['out'])\n",
    "amount_output = builtins.sum([UTXO['value'] for UTXO in TX['out']])\n",
    "print('tx_id:', tx_id)\n",
    "print('tx_hash:', tx_hash)\n",
    "print('fee:', fee)\n",
    "print('n_input:', n_input)\n",
    "print('amount_input:', amount_input)\n",
    "print('n_output:', n_output)\n",
    "print('amount_output:', amount_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JWDkf818zbJi"
   },
   "source": [
    "### **4.1.5.** UTXO's usefull information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Uup6bNDvzkaB",
    "outputId": "46d8d137-dcb1-4198-822f-d9471cb6079b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_UTXO src_position: 4294967295\n",
      "input_UTXO dst_position: 0\n",
      "input_UTXO address: coinbase\n",
      "input_UTXO value: 0\n",
      "output_UTXO src_position: 2098408272645986\n",
      "output_UTXO dst_position: unspent\n",
      "output_UTXO address: 1A1zP1eP5QGefi2DMPTfTL5SLmv7DivfNa\n",
      "output_UTXO value: 5000000000\n"
     ]
    }
   ],
   "source": [
    "#UTXO in input\n",
    "input_UTXO = TXs_list[0]['inputs'][0]\n",
    "src_position = input_UTXO['prev_out']['n']\n",
    "dst_position = input_UTXO['index']\n",
    "try:\n",
    "    address = input_UTXO['prev_out']['addr']\n",
    "except KeyError:\n",
    "    address = 'coinbase'\n",
    "value = input_UTXO['prev_out']['value']\n",
    "print('input_UTXO src_position:', src_position)\n",
    "print('input_UTXO dst_position:', dst_position)\n",
    "print('input_UTXO address:', address)\n",
    "print('input_UTXO value:', value)\n",
    "\n",
    "#UTXO in output\n",
    "output_UTXO = TXs_list[0]['out'][0]\n",
    "src_position = output_UTXO['tx_index']\n",
    "try:\n",
    "    dst_position = output_UTXO['spending_outpoints'][0]['n']\n",
    "except IndexError:\n",
    "    dst_position = 'unspent'\n",
    "address = output_UTXO['addr']\n",
    "value = output_UTXO['value']\n",
    "print('output_UTXO src_position:', src_position)\n",
    "print('output_UTXO dst_position:', dst_position)\n",
    "print('output_UTXO address:', address)\n",
    "print('output_UTXO value:', value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7yjklUaK4vmA"
   },
   "source": [
    "## **4.2.** Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "Ry_X-XaE6SFU"
   },
   "outputs": [],
   "source": [
    "\n",
    "def download_dataset(start_block, end_block, directory, spark_session, debug=False):\n",
    "\n",
    "    if 'blocks-{}-{}'.format(start_block, end_block) in os.listdir(directory):\n",
    "        # dataset is already in directory\n",
    "        d_path = os.path.join(directory, 'blocks-{}-{}'.format(start_block, end_block))\n",
    "        v_path = os.path.join(d_path, 'vertices-{}-{}'.format(start_block, end_block))\n",
    "        e_path = os.path.join(d_path, 'edges-{}-{}'.format(start_block, end_block))\n",
    "        a_path = os.path.join(d_path, 'addresses-{}-{}'.format(start_block, end_block))\n",
    "        if debug: print('dataset is already in {}'.format(d_path))\n",
    "\n",
    "    else:\n",
    "\n",
    "        d_path = os.path.join(directory, 'blocks-{}-{}'.format(start_block, end_block))\n",
    "        os.mkdir(d_path)\n",
    "\n",
    "        try:\n",
    "            # try to dump dataset from github repo datasets\n",
    "            v_path = os.path.join(d_path, 'vertices-{}-{}'.format(start_block, end_block))\n",
    "            e_path = os.path.join(d_path, 'edges-{}-{}'.format(start_block, end_block))\n",
    "            a_path = os.path.join(d_path, 'addresses-{}-{}'.format(start_block, end_block))\n",
    "            wget.download('https://raw.github.com/VincenzoImp/Bitcoin-Address-Clustering/master/dataset/blocks-{}-{}/vertices-{}-{}.tar.gz'.format(start_block, end_block, start_block, end_block), out=d_path)\n",
    "            wget.download('https://raw.github.com/VincenzoImp/Bitcoin-Address-Clustering/master/dataset/blocks-{}-{}/edges-{}-{}.tar.gz'.format(start_block, end_block, start_block, end_block), out=d_path)\n",
    "            wget.download('https://raw.github.com/VincenzoImp/Bitcoin-Address-Clustering/master/dataset/blocks-{}-{}/addresses-{}-{}.tar.gz'.format(start_block, end_block, start_block, end_block), out=d_path)\n",
    "            os.system('tar -xf '+v_path.replace(' ', '\\ ')+'.tar.gz -C '+d_path.replace(' ', '\\ '))\n",
    "            os.system('tar -xf '+e_path.replace(' ', '\\ ')+'.tar.gz -C '+d_path.replace(' ', '\\ '))\n",
    "            os.system('tar -xf '+a_path.replace(' ', '\\ ')+'.tar.gz -C '+d_path.replace(' ', '\\ '))\n",
    "            os.remove(v_path+'.tar.gz')\n",
    "            os.remove(e_path+'.tar.gz')\n",
    "            os.remove(a_path+'.tar.gz')\n",
    "            if debug: print('dataset downloaded from https://github.com/VincenzoImp/Bitcoin-Address-Clustering/master/dataset/blocks-{}-{}'.format(start_block, end_block))\n",
    "\n",
    "        except HTTPError: \n",
    "            # dump dataset manually\n",
    "            v_path = os.path.join(d_path, 'vertices-{}-{}.csv'.format(start_block, end_block))\n",
    "            e_path = os.path.join(d_path, 'edges-{}-{}.csv'.format(start_block, end_block))\n",
    "\n",
    "            if debug: print('dumping blocks...')\n",
    "\n",
    "            v_columns = ['id', 'note', 'tx_hash', 'block_height', 'block_hash', 'fee', 'n_input', 'amount_input', 'n_output', 'amount_output', 'temporal_index']\n",
    "            e_columns = ['src_id', 'dst_id', 'src_position', 'dst_position', 'address', 'value']\n",
    "\n",
    "\n",
    "            temporal_index = 0\n",
    "            count_UXTO = 0\n",
    "\n",
    "            with open(v_path, 'w', encoding='UTF8') as v_file:\n",
    "                with open(e_path, 'w', encoding='UTF8') as e_file:\n",
    "                    \n",
    "                    csv.writer(v_file).writerow(v_columns)\n",
    "                    csv.writer(e_file).writerow(e_columns)\n",
    "                    \n",
    "                    # for each block read all transactions, extract usefull information and write them locally\n",
    "                    for block_height in range(start_block, end_block+1):    \n",
    "\n",
    "                        if debug and block_height % 250 == 0:\n",
    "                            if end_block < block_height+249:\n",
    "                                print('from block {} to block {}'.format(block_height, end_block))\n",
    "                            else:\n",
    "                                print('from block {} to block {}'.format(block_height, block_height+249))\n",
    "\n",
    "                        block_reward = (5000000000 // 2**(block_height//210000))\n",
    "                        # save coinbase information as vertex\n",
    "                        csv.writer(v_file).writerow(['coinbase'+str(block_height), 'coinbase', '', -1, '', 0, 0, 0, 1, block_reward, -1])\n",
    "\n",
    "                        loop = True \n",
    "                        while loop:\n",
    "                            try:\n",
    "                                response = requests.get('https://blockchain.info/block-height/{}'.format(block_height))\n",
    "                                if response.status_code == 200: loop = False\n",
    "                            except HTTPError:\n",
    "                                pass\n",
    "\n",
    "                        block_hash = response.json()['blocks'][0]['hash']\n",
    "\n",
    "                        # read each transaction from a specific json block\n",
    "                        for tx in response.json()['blocks'][0]['tx']:\n",
    "\n",
    "                            tx_id = tx['tx_index']\n",
    "                            tx_hash = tx['hash']\n",
    "                            fee = tx['fee']\n",
    "                            n_input = 0\n",
    "                            amount_input = 0\n",
    "                            n_output = 0\n",
    "                            amount_output = 0\n",
    "\n",
    "                            for incoming_edge in tx['inputs']:\n",
    "                                # consider UTXOs in input\n",
    "                                src_id = incoming_edge['prev_out']['tx_index']\n",
    "                                src_position = incoming_edge['prev_out']['n']\n",
    "\n",
    "                                dst_id = tx_id\n",
    "                                dst_position = incoming_edge['index']\n",
    "\n",
    "                                if src_id == 0:\n",
    "                                    src_id = 'coinbase' + str(block_height)\n",
    "                                    address = 'coinbase' + str(block_height)\n",
    "                                    value = block_reward\n",
    "                                else:\n",
    "                                    try:\n",
    "                                        address = incoming_edge['prev_out']['addr']\n",
    "                                    except KeyError:\n",
    "                                        continue\n",
    "                                    value = incoming_edge['prev_out']['value']\n",
    "\n",
    "                                n_input += 1\n",
    "                                amount_input += value\n",
    "                                \n",
    "                                # save spent UTXO information as edge\n",
    "                                csv.writer(e_file).writerow([str(src_id), str(dst_id), src_position, dst_position, address, value])\n",
    "\n",
    "                            for outgoing_edge in tx['out']:\n",
    "                                # consider UTXOs in output\n",
    "                                src_id = tx_id\n",
    "                                src_position = outgoing_edge['n']\n",
    "\n",
    "                                if outgoing_edge['spending_outpoints'] == []:\n",
    "                                    dst_id = 'UTXO'+str(count_UXTO)\n",
    "                                    count_UXTO += 1\n",
    "                                    dst_position = -1\n",
    "                                else:\n",
    "                                    dst_id = outgoing_edge['spending_outpoints'][0]['tx_index']\n",
    "                                    dst_position = outgoing_edge['spending_outpoints'][0]['n']\n",
    "\n",
    "                                try:\n",
    "                                    address = outgoing_edge['addr']\n",
    "                                except KeyError:\n",
    "                                    continue\n",
    "\n",
    "                                value = outgoing_edge['value']\n",
    "\n",
    "                                n_output += 1\n",
    "                                amount_output += value\n",
    "\n",
    "                                #save spent UTXO information as edge\n",
    "                                csv.writer(e_file).writerow([str(src_id), str(dst_id), src_position, dst_position, address, value])\n",
    "\n",
    "                                if dst_id == 'UTXO'+str(count_UXTO-1):\n",
    "                                    #save unspend UTXO information as vertex\n",
    "                                    csv.writer(v_file).writerow([str(dst_id), 'UTXO', '', -1, '', 0, 1, value, 0, 0, -1])\n",
    "                            \n",
    "                            # save transaction information as vertex\n",
    "                            csv.writer(v_file).writerow([str(tx_id), 'tx', tx_hash, block_height, block_hash, fee, n_input, amount_input, n_output, amount_output, temporal_index])\n",
    "                            temporal_index += 1\n",
    "\n",
    "            # genetare dataframes and save them locally\n",
    "            v_df = spark_session.read.load(v_path, \n",
    "                                    format=\"csv\", \n",
    "                                    sep=\",\", \n",
    "                                    inferSchema=\"true\", \n",
    "                                    header=\"true\"\n",
    "                                    ).distinct()\n",
    "            e_df = spark_session.read.load(e_path, \n",
    "                                    format=\"csv\", \n",
    "                                    sep=\",\", \n",
    "                                    inferSchema=\"true\",\n",
    "                                    header=\"true\"\n",
    "                                    ).distinct()\n",
    "\n",
    "            e_df.createOrReplaceTempView('EDGES')\n",
    "            a_df = e_df.select('address').subtract(spark_session.sql(\"select address from EDGES where address like 'coinbase%'\"))\n",
    "            a_df = a_df.withColumn('cluster_id', monotonically_increasing_id())\n",
    "\n",
    "            v_path = os.path.join(d_path, 'vertices-{}-{}'.format(start_block, end_block))\n",
    "            e_path = os.path.join(d_path, 'edges-{}-{}'.format(start_block, end_block))\n",
    "            a_path = os.path.join(d_path, 'addresses-{}-{}'.format(start_block, end_block))\n",
    "\n",
    "            v_df.write.save(path=v_path, format='csv', header='True')\n",
    "            e_df.write.save(path=e_path, format='csv', header='True')\n",
    "            a_df.write.save(path=a_path, format='csv', header='True')\n",
    "\n",
    "            os.remove(os.path.join(d_path, 'vertices-{}-{}.csv'.format(start_block, end_block)))\n",
    "            os.remove(os.path.join(d_path, 'edges-{}-{}.csv'.format(start_block, end_block)))\n",
    "\n",
    "            if debug: print('dataset downloaded')\n",
    "    \n",
    "    # return dataset files paths\n",
    "    return v_path, e_path, a_path, d_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gXMjcOP_Q_6-",
    "outputId": "e7c93e2c-e685-431a-b4b1-d08d66c4b61e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset downloaded from https://github.com/VincenzoImp/Bitcoin-Address-Clustering/master/dataset/blocks-0-120000\n"
     ]
    }
   ],
   "source": [
    "v_path, e_path, a_path, d_path = download_dataset(start_block, end_block, GDRIVE_DATA_DIR, spark, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "PaoKRRzuN9xN"
   },
   "outputs": [],
   "source": [
    "v_df = spark.read.load(v_path, \n",
    "                         format=\"csv\", \n",
    "                         sep=\",\", \n",
    "                         inferSchema=\"true\", \n",
    "                         header=\"true\"\n",
    "                         )\n",
    "e_df = spark.read.load(e_path, \n",
    "                         format=\"csv\", \n",
    "                         sep=\",\", \n",
    "                         inferSchema=\"true\", \n",
    "                         header=\"true\"\n",
    "                         )\n",
    "a_df = spark.read.load(a_path, \n",
    "                         format=\"csv\", \n",
    "                         sep=\",\", \n",
    "                         inferSchema=\"true\", \n",
    "                         header=\"true\"\n",
    "                         )\n",
    "unknown_v_df = e_df.select('src_id').union(e_df.select('dst_id')).distinct().subtract(v_df.select('id')).withColumnRenamed('src_id', 'id')\n",
    "unknown_v_df = unknown_v_df.withColumn('note', lit('unknown_tx')) \\\n",
    "                          .withColumn('tx_hash', lit(None)) \\\n",
    "                          .withColumn('block_height', lit(-1)) \\\n",
    "                          .withColumn('block_hash', lit(None)) \\\n",
    "                          .withColumn('fee', lit(0)) \\\n",
    "                          .withColumn('n_input', lit(0)) \\\n",
    "                          .withColumn('amount_input', lit(0)) \\\n",
    "                          .withColumn('n_output', lit(0)) \\\n",
    "                          .withColumn('amount_output', lit(0)) \\\n",
    "                          .withColumn('temporal_index', lit(-1))\n",
    "                          \n",
    "v_df = v_df.union(unknown_v_df)\n",
    "\n",
    "v_df.createOrReplaceTempView('VERTICES')\n",
    "known_tx_df = spark.sql(\"select * from VERTICES where note = 'tx'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "31g2CVI3da55",
    "outputId": "42415758-6876-46b8-f8a3-225a60062651"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total nodes: 644770\n",
      "Total edges: 774633\n",
      "Total addresses: 416792\n",
      "Total known transactions: 435536\n"
     ]
    }
   ],
   "source": [
    "print(\"Total nodes: {}\".format(v_df.count()))\n",
    "print(\"Total edges: {}\".format(e_df.count()))\n",
    "print(\"Total addresses: {}\".format(a_df.count()))\n",
    "print(\"Total known transactions: {}\".format(known_tx_df.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XkGHDOcddjjV",
    "outputId": "964f98ff-4c93-419b-c3b2-c3de7eed8e1c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- note: string (nullable = true)\n",
      " |-- tx_hash: string (nullable = true)\n",
      " |-- block_height: integer (nullable = true)\n",
      " |-- block_hash: string (nullable = true)\n",
      " |-- fee: integer (nullable = true)\n",
      " |-- n_input: integer (nullable = true)\n",
      " |-- amount_input: long (nullable = true)\n",
      " |-- n_output: integer (nullable = true)\n",
      " |-- amount_output: long (nullable = true)\n",
      " |-- temporal_index: integer (nullable = true)\n",
      "\n",
      "root\n",
      " |-- src_id: string (nullable = true)\n",
      " |-- dst_id: string (nullable = true)\n",
      " |-- src_position: long (nullable = true)\n",
      " |-- dst_position: integer (nullable = true)\n",
      " |-- address: string (nullable = true)\n",
      " |-- value: long (nullable = true)\n",
      "\n",
      "root\n",
      " |-- address: string (nullable = true)\n",
      " |-- cluster_id: long (nullable = true)\n",
      "\n",
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- note: string (nullable = true)\n",
      " |-- tx_hash: string (nullable = true)\n",
      " |-- block_height: integer (nullable = true)\n",
      " |-- block_hash: string (nullable = true)\n",
      " |-- fee: integer (nullable = true)\n",
      " |-- n_input: integer (nullable = true)\n",
      " |-- amount_input: long (nullable = true)\n",
      " |-- n_output: integer (nullable = true)\n",
      " |-- amount_output: long (nullable = true)\n",
      " |-- temporal_index: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "v_df.printSchema()\n",
    "e_df.printSchema()\n",
    "a_df.printSchema()\n",
    "known_tx_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zgzyaju3dp3B",
    "outputId": "44b5b8d3-fb90-4f23-a3cf-e61886236203"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------+----------------------------------------------------------------+------------+----------------------------------------------------------------+---+-------+------------+--------+-------------+--------------+\n",
      "|id              |note    |tx_hash                                                         |block_height|block_hash                                                      |fee|n_input|amount_input|n_output|amount_output|temporal_index|\n",
      "+----------------+--------+----------------------------------------------------------------+------------+----------------------------------------------------------------+---+-------+------------+--------+-------------+--------------+\n",
      "|coinbase144     |coinbase|null                                                            |-1          |null                                                            |0  |0      |0           |1       |5000000000   |-1            |\n",
      "|UTXO173         |UTXO    |null                                                            |-1          |null                                                            |0  |1      |5000000000  |0       |0            |-1            |\n",
      "|UTXO186         |UTXO    |null                                                            |-1          |null                                                            |0  |1      |5000000000  |0       |0            |-1            |\n",
      "|UTXO252         |UTXO    |null                                                            |-1          |null                                                            |0  |1      |5000000000  |0       |0            |-1            |\n",
      "|563524614200817 |tx      |cfca74c0c3b1f8560fbc3866915a161d4fc1c03b85acb9ef3a8bef9c632e0410|353         |00000000dec5e5fbf851495db241e624012d675bbef96ec6dab5cdc91fa3b53b|0  |1      |5000000000  |1       |5000000000   |360           |\n",
      "|UTXO390         |UTXO    |null                                                            |-1          |null                                                            |0  |1      |5000000000  |0       |0            |-1            |\n",
      "|coinbase480     |coinbase|null                                                            |-1          |null                                                            |0  |0      |0           |1       |5000000000   |-1            |\n",
      "|UTXO476         |UTXO    |null                                                            |-1          |null                                                            |0  |1      |5000000000  |0       |0            |-1            |\n",
      "|coinbase716     |coinbase|null                                                            |-1          |null                                                            |0  |0      |0           |1       |5000000000   |-1            |\n",
      "|UTXO743         |UTXO    |null                                                            |-1          |null                                                            |0  |1      |5000000000  |0       |0            |-1            |\n",
      "|881193180662994 |tx      |4b4642c2f07835ca1299b7085c08d8702d66e659ff8dde9e6091661834860b19|905         |00000000a8ed265d3d7d4a829dac44c97b30f1a519bcd7ca038a7c680755a30c|0  |1      |5000000000  |1       |5000000000   |923           |\n",
      "|2515756714985193|tx      |d50f65db5bcb69874642e25c0822e8e25bd0d12d08ed49bb124e37b40a8a8047|1081        |00000000a748cbe2ec8fca05e61699e970173ab54dd13737a69a514fce64c3ee|0  |1      |5000000000  |1       |5000000000   |1102          |\n",
      "|UTXO992         |UTXO    |null                                                            |-1          |null                                                            |0  |1      |5000000000  |0       |0            |-1            |\n",
      "|UTXO1049        |UTXO    |null                                                            |-1          |null                                                            |0  |1      |5000000000  |0       |0            |-1            |\n",
      "|3690346203056177|tx      |bbf9d4ea04d443211005058bb1697dbe17936016adc6ae49d58e414074cde268|1181        |00000000f39741da98b6f3401782b09fcfe02dba0ef247ac9f3342b70db463e1|0  |1      |5000000000  |1       |5000000000   |1202          |\n",
      "|1886044634084161|tx      |3db445c1fbce899ea4286b8e80afaf5dd932393a39c55652a20e9a8fe3c79a35|1436        |00000000c6bdeda56c538ca9a63a2f0e46c5e3df8c0eac955300f1964f5cb1a8|0  |1      |5000000000  |1       |5000000000   |1461          |\n",
      "|UTXO1400        |UTXO    |null                                                            |-1          |null                                                            |0  |1      |5000000000  |0       |0            |-1            |\n",
      "|coinbase1615    |coinbase|null                                                            |-1          |null                                                            |0  |0      |0           |1       |5000000000   |-1            |\n",
      "|coinbase1695    |coinbase|null                                                            |-1          |null                                                            |0  |0      |0           |1       |5000000000   |-1            |\n",
      "|UTXO1608        |UTXO    |null                                                            |-1          |null                                                            |0  |1      |5000000000  |0       |0            |-1            |\n",
      "+----------------+--------+----------------------------------------------------------------+------------+----------------------------------------------------------------+---+-------+------------+--------+-------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+----------------+----------------+------------+------------+----------------------------------+----------+\n",
      "|src_id          |dst_id          |src_position|dst_position|address                           |value     |\n",
      "+----------------+----------------+------------+------------+----------------------------------+----------+\n",
      "|3628106370150103|UTXO89          |0           |-1          |1BnhHuk62o9tPuG5JerexTX3MhnR8sobLm|5000000000|\n",
      "|coinbase216     |56554460906045  |4294967295  |0           |coinbase216                       |5000000000|\n",
      "|3355981834706124|UTXO270         |0           |-1          |1AKQFY7YcDFj4xpGpYQ2GnpyGu1YGyUHcF|5000000000|\n",
      "|coinbase348     |6638960124589330|4294967295  |0           |coinbase348                       |5000000000|\n",
      "|8353454767917996|UTXO406         |0           |-1          |1LWsDG5tyyRrKg7a8YvmACD34ovxX514aA|5000000000|\n",
      "|7522455066869868|UTXO429         |0           |-1          |1CtV9nooWqR8zYbN3PnetnhmkFEZiCWmCP|5000000000|\n",
      "|8940088839617268|UTXO431         |0           |-1          |1Q7Ach95Kv3eFf5DxTNTsgMxMhBFtJtXuf|5000000000|\n",
      "|4880392915581309|UTXO583         |0           |-1          |1E3ENaJ5UiVWHyzHvDa3jpnj1RUaKgdF4T|5000000000|\n",
      "|5935868801175383|UTXO890         |0           |-1          |12aEVs382mMTcVDrRGj9FhkXFeHaeyL5Jr|5000000000|\n",
      "|2437121443161923|UTXO1005        |0           |-1          |15zzrk3b71DWRBJ3mk36YbLAtUUJnYFbhy|5000000000|\n",
      "|coinbase1126    |1573022456004800|4294967295  |0           |coinbase1126                      |5000000000|\n",
      "|coinbase1295    |4039766567270086|4294967295  |0           |coinbase1295                      |5000000000|\n",
      "|coinbase1419    |7927349273666572|4294967295  |0           |coinbase1419                      |5000000000|\n",
      "|1793643441614570|UTXO1286        |0           |-1          |156UHojM4GC9HmqM8tmeWk7RhAuhprFXbo|5000000000|\n",
      "|coinbase1605    |2343347440708146|4294967295  |0           |coinbase1605                      |5000000000|\n",
      "|427613614425043 |UTXO1471        |0           |-1          |1Mj84GjfCsk3Cxv8T2WP1KaMkkDfJm3L9Q|5000000000|\n",
      "|coinbase1759    |1537057447335656|4294967295  |0           |coinbase1759                      |5000000000|\n",
      "|7266922639802432|UTXO1667        |0           |-1          |1FEZSvooD1vaa8pDxz3hVMmzarjL7J8zUo|5000000000|\n",
      "|coinbase2302    |6170794455668463|4294967295  |0           |coinbase2302                      |5000000000|\n",
      "|6388245315065960|UTXO2130        |0           |-1          |1QHBKxHoPqZgvcp8AqYb5ictoiwiQzS5ZB|5000000000|\n",
      "+----------------+----------------+------------+------------+----------------------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+----------------------------------+-----------+\n",
      "|address                           |cluster_id |\n",
      "+----------------------------------+-----------+\n",
      "|1NZz1UN616LfyX87zGRnVifiWRSN5xm9ht|60129542144|\n",
      "|1M25afExu5qjfC5gaeAhjfzNnDDLG5XaBy|60129542145|\n",
      "|1QGmrvZTj3gFgTmVGVJypsVzXQAsnsTvfW|60129542146|\n",
      "|1EtvzKQpXyqswKopcvMheAYDysFA1ajRWN|60129542147|\n",
      "|19qmNYvQxEn7pKYQZGzsi1QcutMXPmDAT1|60129542148|\n",
      "|12CbDsSLXTDSwYiP13JZXvtfAbheLFmQ4T|60129542149|\n",
      "|17zsdkdkkEWqznp9Xz9KreM6oF3FY9jCEa|60129542150|\n",
      "|1MBBJBFEaYKHFZAeV7hQ7DWdu3aZktjzFH|60129542151|\n",
      "|1HDRNeAq9ZS8Zt4UZAu4ogkSaAFcTkv1aP|60129542152|\n",
      "|1CbAopr7sNMKjsJa8dAS6P5sM788kmMooo|60129542153|\n",
      "|1EEbC13oFas6hLXh5WvPjJd1DZfoagd5os|60129542154|\n",
      "|1M59ddtkgQuaHAgX96No2FKg1XjARV515i|60129542155|\n",
      "|1CUKvVHDga1yhs2HbXZH4FT5Xi6VdqoZgM|60129542156|\n",
      "|1KqYMG4Mzdy3z7bmNN2zpEyyFNLWum7MSB|60129542157|\n",
      "|18xu1Xu1B9nh8RxHVWDeywvnk4ED9Hg9cH|60129542158|\n",
      "|1N7GVNgJcYnqdV7niH6zN8nSy7wL5Uf1fJ|60129542159|\n",
      "|1BjpESo9nYoqLUp58YoqsnGSyHJYNqp9T9|60129542160|\n",
      "|1HGDYFtZGmNTCaNcq29i1Aj97C9APTSmvK|60129542161|\n",
      "|1Eq1HBuEVzHypp82UGQ4Cdeaanq4g3wVam|60129542162|\n",
      "|1FM6RwfL6NeWJuq5qbAPnaKdU1ELLhzd18|60129542163|\n",
      "+----------------------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+----------------+----+----------------------------------------------------------------+------------+----------------------------------------------------------------+---+-------+------------+--------+-------------+--------------+\n",
      "|id              |note|tx_hash                                                         |block_height|block_hash                                                      |fee|n_input|amount_input|n_output|amount_output|temporal_index|\n",
      "+----------------+----+----------------------------------------------------------------+------------+----------------------------------------------------------------+---+-------+------------+--------+-------------+--------------+\n",
      "|563524614200817 |tx  |cfca74c0c3b1f8560fbc3866915a161d4fc1c03b85acb9ef3a8bef9c632e0410|353         |00000000dec5e5fbf851495db241e624012d675bbef96ec6dab5cdc91fa3b53b|0  |1      |5000000000  |1       |5000000000   |360           |\n",
      "|881193180662994 |tx  |4b4642c2f07835ca1299b7085c08d8702d66e659ff8dde9e6091661834860b19|905         |00000000a8ed265d3d7d4a829dac44c97b30f1a519bcd7ca038a7c680755a30c|0  |1      |5000000000  |1       |5000000000   |923           |\n",
      "|2515756714985193|tx  |d50f65db5bcb69874642e25c0822e8e25bd0d12d08ed49bb124e37b40a8a8047|1081        |00000000a748cbe2ec8fca05e61699e970173ab54dd13737a69a514fce64c3ee|0  |1      |5000000000  |1       |5000000000   |1102          |\n",
      "|3690346203056177|tx  |bbf9d4ea04d443211005058bb1697dbe17936016adc6ae49d58e414074cde268|1181        |00000000f39741da98b6f3401782b09fcfe02dba0ef247ac9f3342b70db463e1|0  |1      |5000000000  |1       |5000000000   |1202          |\n",
      "|1886044634084161|tx  |3db445c1fbce899ea4286b8e80afaf5dd932393a39c55652a20e9a8fe3c79a35|1436        |00000000c6bdeda56c538ca9a63a2f0e46c5e3df8c0eac955300f1964f5cb1a8|0  |1      |5000000000  |1       |5000000000   |1461          |\n",
      "|5277687267313419|tx  |16f33c8670289de8c8519928eb534942e54fbc237cdca7b3a659386e963a0096|2010        |00000000612cddac3513ac8a59168132e07dcf56690f6da51b76663a9182df2b|0  |1      |5000000000  |1       |5000000000   |2040          |\n",
      "|8593499483164245|tx  |291000e94bdcb65db39f0faf3a359e4de28d09ac5aa725a0a0ae729220f03df4|2636        |00000000438a2e8e2e9e132dfa69a6a67704c57bfb3cc2930349260ec05d3486|0  |1      |5000000000  |1       |5000000000   |2669          |\n",
      "|3673557673204206|tx  |609ad70c2ffba5b2ac11cd1525ce33be7e6f2fe0bedee0880c77cf6261a66868|2940        |00000000c03c0de143abe860076d2a0ffa92f9f1dc08eb0c94c6bc3b8998e27a|0  |1      |5000000000  |1       |5000000000   |2990          |\n",
      "|8776194308097506|tx  |229917bd80f2284dd7a22559749d2e3e4cdc8bc9fb96e4f1e4122fb6c1376ff9|2994        |00000000f6a31af28cfe8839f21bf7111283bedb001373f206f86067303355bc|0  |1      |5000000000  |1       |5000000000   |3044          |\n",
      "|5942645310990394|tx  |31176f8d969e2f5f26882ff3eabeeed283f9b56b7b8b478e45d3414d766fe6a8|3131        |000000003b4f4c304e80a5fbdbaf200ae4a910bd5432aae07ea0cf45e03b76a3|0  |1      |5000000000  |1       |5000000000   |3181          |\n",
      "|3710436898114488|tx  |4310b3b8bf9591e3ceb66e44b9b4a0e556a34ba63835a1e6d0c35d304afb7469|3134        |000000001ee955645635bf9569c048ca034efef0bb5877f24767c310ea33fd23|0  |1      |5000000000  |1       |5000000000   |3184          |\n",
      "|96783619374603  |tx  |e12d3f5184c4fe2f3908a116785b1c32590c028bee9910dc6d59f0058a31c002|3507        |00000000414f125d89e439ff1bf0054ddda03749674e936ed0471d8760752994|0  |1      |5000000000  |1       |5000000000   |3561          |\n",
      "|2278267292002436|tx  |fe7008b3a31416717d7df128902c884b4dd1ba4bf133d90bba2144258593c040|3881        |00000000c723fcfb145646bd41a0344c02e2a6e490d0fc15bee1c5be5ddc404e|0  |1      |5000000000  |1       |5000000000   |3937          |\n",
      "|5195181229050095|tx  |f340d36663a01bd8fdd1203cc48a8310404ea66f1bd8054e6c7f479c1deba793|4037        |0000000045c689dc49dee778a9fbca7b5bc48fceca9f05cde5fc8d667f00e7d2|0  |1      |5000000000  |1       |5000000000   |4095          |\n",
      "|5844129209073959|tx  |1619902e5c581c2d44c1ad117d5134ca199ee80d9ce289a6823e093aeca219a6|4624        |0000000055156abeea523f65997cbf0a950a8364d3bda7ef1ea190dfd3842761|0  |1      |5000000000  |1       |5000000000   |4686          |\n",
      "|3914222767566058|tx  |db592dd90e53588cac5983b4aec0bd01b0e02b7bbdc2b8cc1f53670f0db83f6f|4678        |00000000f460589c3a045fa38573c8d3e0688db64b052bb238bd5854b1db1db6|0  |1      |5000000000  |1       |5000000000   |4740          |\n",
      "|1246176799551415|tx  |936ed21eae7ade9d51aaa52c8e73dddbba5f44c61b6736701fbf7db72b216b23|4840        |000000002e43d0746650ecdbea7fe84bf70a08d2a9e876aaa2b61a65065dc73e|0  |1      |5000000000  |1       |5000000000   |4902          |\n",
      "|445742614571975 |tx  |f0e10d5bb31b182eb8d7509fd16490237345c3be2ad0a1376d3c5e9f5134ab0c|4930        |00000000696b0f25ca63595ee901c059ca94ebb99ce16a84c421a8565f86c77a|0  |1      |5000000000  |1       |5000000000   |4993          |\n",
      "|8735005524357819|tx  |326d2a34ff8a56d9b2b7a6806b60040595a4c06ccb81873476d9951fab8743f8|5163        |000000003dc1c00b7d364fbdaf620be15e6d6d9f2ef6129e9569075460902f10|0  |1      |5000000000  |1       |5000000000   |5226          |\n",
      "|2040783629645809|tx  |33de74a344be8233d7f0df02172cd7736f9af9110c4537209d8bbf77baa7003a|5299        |00000000929fa878ae228c8df06d15f897d2c33e9bafd7932dbe0604c4944ca6|0  |1      |5000000000  |1       |5000000000   |5366          |\n",
      "+----------------+----+----------------------------------------------------------------+------------+----------------------------------------------------------------+---+-------+------------+--------+-------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "v_df.show(truncate=False)\n",
    "e_df.show(truncate=False)\n",
    "a_df.show(truncate=False)\n",
    "known_tx_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ekR20IG8B9R"
   },
   "source": [
    "## **4.3.** Transaction Graph generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "RBjC9lzpUcCm"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "import os\n",
    "import networkx as nx\n",
    "'''\n",
    "\n",
    "def generate_nx_graph(v_df, e_df, graph_path, start_block, end_block, debug=False):\n",
    "\n",
    "    graph_name = os.path.basename(graph_path)\n",
    "    d_path = os.path.dirname(graph_path)\n",
    "    if graph_name in os.listdir(os.path.dirname(graph_path)):\n",
    "        # graph is already in directory\n",
    "        G = nx.read_gexf(graph_path)\n",
    "        G = nx.MultiDiGraph(G)\n",
    "        if debug: print('graph is already in {}'.format(d_path))\n",
    "    else:\n",
    "        try:\n",
    "            # try to dump graph from github repo datasets\n",
    "            request.urlretrieve('https://raw.github.com/VincenzoImp/Bitcoin-Address-Clustering/master/dataset/blocks-{}-{}/{}'.format(start_block, end_block, graph_name), graph_path)\n",
    "            G = nx.read_gexf(graph_path)\n",
    "            G = nx.MultiDiGraph(G)\n",
    "            if debug: print('graph downloaded from https://github.com/VincenzoImp/Bitcoin-Address-Clustering/master/dataset/blocks-{}-{}'.format(start_block, end_block))\n",
    "        except:\n",
    "            # generate graph manually\n",
    "            if debug: print('generating graph...')\n",
    "            G = nx.MultiDiGraph()\n",
    "\n",
    "            for node in v_df.rdd.collect():\n",
    "                if node.note == 'tx':\n",
    "                    G.add_node(\n",
    "                        node_for_adding=str(node.id), \n",
    "                        note=node.note, \n",
    "                        tx_hash=node.tx_hash, \n",
    "                        block_height=node.block_height,\n",
    "                        fee=node.fee/100000000,\n",
    "                        n_input=node.n_input,\n",
    "                        amount_input=node.amount_input/100000000,\n",
    "                        n_output=node.n_output,\n",
    "                        amount_output=node.amount_output/100000000\n",
    "                        )\n",
    "                elif node.note == 'UTXO':\n",
    "                    G.add_node(\n",
    "                        node_for_adding=str(node.id), \n",
    "                        note=node.note,\n",
    "                        amount=node.amount_input/100000000\n",
    "                    )\n",
    "                elif node.note == 'coinbase':\n",
    "                    G.add_node(\n",
    "                        node_for_adding=str(node.id), \n",
    "                        note=node.note,\n",
    "                        amount=node.amount_output/100000000\n",
    "                    )\n",
    "                elif node.note == 'unknown_tx':\n",
    "                    G.add_node(\n",
    "                        node_for_adding=str(node.id), \n",
    "                        note=node.note\n",
    "                    )\n",
    "\n",
    "            for edge in e_df.rdd.collect():\n",
    "                G.add_edge(\n",
    "                    u_for_edge=str(edge.src_id),\n",
    "                    v_for_edge=str(edge.dst_id), \n",
    "                    key=None,\n",
    "                    address=edge.address,\n",
    "                    value=edge.value/100000000,\n",
    "                )\n",
    "\n",
    "            nx.write_gexf(G, graph_path)\n",
    "            if debug: print('graph generated')\n",
    "\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dNK57chkieVP",
    "outputId": "2a97cb25-7bb3-4beb-881b-26cda481ce8e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph downloaded from https://github.com/VincenzoImp/Bitcoin-Address-Clustering/master/dataset/blocks-0-120000\n"
     ]
    }
   ],
   "source": [
    "graph_path = os.path.join(d_path, 'graph-{}-{}.gexf.bz2'.format(start_block, end_block))\n",
    "nx_graph = generate_nx_graph(v_df, e_df, graph_path, start_block, end_block, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "woABEepU7I3x"
   },
   "source": [
    "## **4.4.** Look inside the blockchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eIilhZlc706W",
    "outputId": "30a5aa23-6d7c-4509-c13b-864f8cf81922"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total nodes: 644770\n",
      "Total edges: 774633\n",
      "Total addresses: 416792\n",
      "Total known transactions: 435536\n"
     ]
    }
   ],
   "source": [
    "print(\"Total nodes: {}\".format(v_df.count()))\n",
    "print(\"Total edges: {}\".format(e_df.count()))\n",
    "print(\"Total addresses: {}\".format(a_df.count()))\n",
    "print(\"Total known transactions: {}\".format(known_tx_df.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LS2PfDxFb4CC"
   },
   "source": [
    "### **4.4.1.** Connected Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v5oJkOpn7_tx"
   },
   "outputs": [],
   "source": [
    "cc = [nx_graph.subgraph(c).copy() for c in nx.connected_components(nx_graph.to_undirected())]\n",
    "tmp_list = [(i, len(c.nodes()), len(c.edges())) for i, c in enumerate(cc)]\n",
    "cc_df = spark.createDataFrame(data = tmp_list, schema = ['id', 'num_nodes', 'num_edges'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9UrpA8G8MYFj"
   },
   "outputs": [],
   "source": [
    "print(cc_df.count())\n",
    "cc_df.printSchema()\n",
    "cc_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ooga4vtCOJU_"
   },
   "outputs": [],
   "source": [
    "cc_df.createOrReplaceTempView('CC')\n",
    "statistics_cc_df = spark.sql(\"select num_nodes, num_edges, count(num_nodes, num_edges) as n_components from CC group by num_nodes, num_edges order by num_nodes, num_edges\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EbqRCi3PRmwT"
   },
   "outputs": [],
   "source": [
    "print(statistics_cc_df.count())\n",
    "statistics_cc_df.printSchema()\n",
    "statistics_cc_df.show(statistics_cc_df.count(), truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PnNrIAcRVrGg"
   },
   "outputs": [],
   "source": [
    "plt.scatter(\n",
    "    x = statistics_cc_df.toPandas()['num_nodes'], \n",
    "    y = statistics_cc_df.toPandas()['num_edges'], \n",
    "    s=statistics_cc_df.toPandas()['n_components'], \n",
    "    alpha=0.5)\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.xlabel(\"num_nodes\")\n",
    "plt.ylabel(\"num_edges\")\n",
    "plt.title(\"Connected components distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xid6RlGNcACt"
   },
   "source": [
    "### **4.4.2.** Indegree and Outdegree of Known Transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JR2Wu6XH9Enp"
   },
   "outputs": [],
   "source": [
    "in_degree_df = spark.createDataFrame(nx_graph.in_degree(nx_graph.nodes()), schema=['id1', 'in_degree'])\n",
    "out_degree_df = spark.createDataFrame(nx_graph.out_degree(nx_graph.nodes()), schema=['id2', 'out_degree'])\n",
    "known_tx_degree_df = in_degree_df.join(out_degree_df, in_degree_df.id1 == out_degree_df.id2, 'inner')\n",
    "known_tx_degree_df = known_tx_degree_df.join(known_tx_df, known_tx_degree_df.id1 == known_tx_df.id, 'inner')\n",
    "known_tx_degree_df = known_tx_degree_df.select('id', 'in_degree', 'out_degree')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m4uB8Eg0onrw"
   },
   "outputs": [],
   "source": [
    "print(known_tx_degree_df.count())\n",
    "known_tx_degree_df.printSchema()\n",
    "known_tx_degree_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "piHMGhQlyfIS"
   },
   "outputs": [],
   "source": [
    "known_tx_degree_df.createOrReplaceTempView('KNOWN_TX_DEGREE')\n",
    "statistics_known_tx_degree_df = spark.sql(\"select in_degree, out_degree, count(in_degree, out_degree) as n_txs from KNOWN_TX_DEGREE group by in_degree, out_degree order by in_degree, out_degree\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xOrWM70Sy4-D"
   },
   "outputs": [],
   "source": [
    "print(statistics_known_tx_degree_df.count())\n",
    "statistics_known_tx_degree_df.printSchema()\n",
    "statistics_known_tx_degree_df.show(statistics_known_tx_degree_df.count(), truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VIlqa2Q80iTC"
   },
   "outputs": [],
   "source": [
    "plt.scatter(\n",
    "    x = statistics_known_tx_degree_df.toPandas()['in_degree'], \n",
    "    y = statistics_known_tx_degree_df.toPandas()['out_degree'], \n",
    "    s=statistics_known_tx_degree_df.toPandas()['n_txs'], \n",
    "    alpha=0.5)\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.xlabel(\"in_degree\")\n",
    "plt.ylabel(\"out_degree\")\n",
    "plt.title(\"Indegree and Outdegree of Known Transactions distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SuSKOARPYlqb"
   },
   "outputs": [],
   "source": [
    "statistics_known_tx_degree_df.createOrReplaceTempView('STAT_KNOWN_TX_DEGREE')\n",
    "tmp_df = spark.sql(\"select out_degree, sum(n_txs) as n_txs from STAT_KNOWN_TX_DEGREE group by out_degree order by out_degree\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BAeiVrN4e5U4"
   },
   "outputs": [],
   "source": [
    "tmp_df.show(tmp_df.count(), truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KPKCfh6sjSge"
   },
   "outputs": [],
   "source": [
    "statistics_known_tx_degree_df.createOrReplaceTempView('STAT_KNOWN_TX_DEGREE')\n",
    "tmp_df = spark.sql(\"select in_degree, sum(n_txs) as n_txs from STAT_KNOWN_TX_DEGREE group by in_degree order by in_degree\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LHodQzvjjeP_"
   },
   "outputs": [],
   "source": [
    "tmp_df.show(tmp_df.count(), truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t4-FiNF8MNF7"
   },
   "source": [
    "## **4.5.** Implementation of Heuristics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3cA2Yp_lMkOt"
   },
   "source": [
    "### **4.5.1.** Multiple interpretations of a blockchain transaction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UY2Yek9gS0TA"
   },
   "source": [
    "Bitcoin transactions are made up of inputs and outputs, of which there can be one or more. Previously-created outputs can be used as inputs for later transactions. Such outputs are destroyed when spent and new unspent outputs are usually created to replace them.\n",
    "\n",
    "Consider this example transaction:\n",
    "\n",
    "    1 btc  ---->  1 btc \n",
    "    3 btc         3 btc\n",
    "This transaction has two inputs, worth 1 btc and 3 btc, and creates two outputs also worth 1 btc and 3 btc.\n",
    "\n",
    "If you were to look at this on the blockchain, what would you assume is the meaning of this transaction? (for example, we usually assume a bitcoin transaction is a payment but it doesn't have to be).\n",
    "\n",
    "There are at least nine' possible interpretations:\n",
    "\n",
    "1. Alice provides both inputs and pays 3 btc to Bob. Alice owns the 1 btc output (i.e. it is a change output).\n",
    "2. Alice provides both inputs and pays 1 btc to Bob, with 3 btc paid back to Alice as the change.\n",
    "3. Alice provides 1 btc input and Bob provides 3 btc input, Alice gets 1 btc output and Bob gets 3 btc output. This is a kind of CoinJoin transaction.\n",
    "4. Alice pays 2 btc to Bob. Alice provides 3 btc input, gets the 1 btc output; Bob provides 1 btc input and gets 3 btc. This would be a PayJoin transaction type.\n",
    "5. Alice pays 4 btc to Bob (but using two outputs for some reason).\n",
    "6. Fake transaction - Alice owns all inputs and outputs, and is simply moving coins between her own addresses.\n",
    "7. Alice pays Bob 3 btc and Carol 1 btc. This is a batched payment with no change address.\n",
    "8. Alice pays 3, Bob pays 1; Carol gets 3 btc and David gets 1 btc. This is some kind of CoinJoined batched payment with no change address.\n",
    "9. Alice and Bob pay 4 btc to Carol (but using two outputs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_4UVPPIDPnio"
   },
   "source": [
    "### **4.5.2.** Heuristics Implemented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kP_5Yx88ISqc"
   },
   "source": [
    "sathoshi\n",
    "\n",
    "bitcoin pizza day\n",
    "\n",
    "common-input-ownership-heuristic per consolidatioon di tx con 1 o 2 output\n",
    "\n",
    "mining address \n",
    "\n",
    "_____\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BWUXFC_bQh9q"
   },
   "source": [
    "#### Satoshi heuristic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oOEs7CZ7VUb9"
   },
   "source": [
    "First 10500 blocks are minted by Satoshi. First Satoshi's address: 1A1zP1eP5QGefi2DMPTfTL5SLmv7DivfNa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aIE55Sy6QScz"
   },
   "source": [
    "#### Common-input-ownership heuristic\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SVzdCd3xSZUK"
   },
   "source": [
    "This is a heuristic or assumption which says that if a transaction has more than one input then all those inputs are owned by the same entity.\n",
    "For example, consider this transaction with inputs A, B and C; and outputs X and Y.\n",
    "\n",
    "    A (1 btc) --> X (4 btc)\n",
    "    B (2 btc)     Y (2 btc)\n",
    "    C (3 btc)\n",
    "This transaction would be an indication that addresses B and C are owned by the same person who owns address A.\n",
    "One of the purposes of CoinJoin is to break this heuristic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VVb9OwQUQ1Ee"
   },
   "source": [
    "#### Coinbase transaction mining address clustering heuristic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zzUlJMWBtv3W"
   },
   "source": [
    "We can assume that the output address of a Coinbase transaction is\n",
    "controlled by the same entity. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dnJtFCNiQ1YA"
   },
   "source": [
    "#### Mixed transaction recognition heuristic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3zns64Y8uEyh"
   },
   "source": [
    "bla bla"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FbdK2SD1Q03w"
   },
   "source": [
    "#### Change address detection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pxceHGi-SgEL"
   },
   "source": [
    "Many bitcoin transactions have change outputs. It would be a serious privacy leak if the change address can be somehow found, as it would link the ownership of the (now spent) inputs with a new output. Change outputs can be very effective when combined with other privacy leaks like the common-input-ownership heuristic or address reuse. Change address detection allows the adversary to cluster together newly created address, which the common-input-ownership heuristic and address reuse allows past addresses to be clustered.\n",
    "\n",
    "Change addresses lead to a common usage pattern called the peeling chain. It is seen after a large transactions from exchanges, marketplaces, mining pools and salary payments. In a peeling chain, a single address begins with a relatively large amount of bitcoins. A smaller amount is then peeled off this larger amount, creating a transaction in which a small amount is transferred to one address, and the remainder is transferred to a one-time change address. This process is repeated - potentially for hundreds or thousands of hops - until the larger amount is pared down, at which point (in one usage) the amount remaining in the address might be aggregated with other such addresses to again yield a large amount in a single address, and the peeling process begins again[5].\n",
    "\n",
    "Now are listed possible ways to infer which of the outputs of a transaction is the change output:\n",
    "\n",
    "Address reuse\n",
    "If an output address has been reused it is very likely to be a payment output, not a change output. This is because change addresses are created automatically by wallet software but payment addresses are manually sent between humans. The address reuse would happen because the human user reused an address out of ignorance or apathy. This heuristic is probably the most accurate, as it is very hard to imagine how false positives would arise (except by intentional design of wallets). This heuristic is also called the \"shadow heuristic\".\n",
    "\n",
    "Some very old software (from the 2010-2011 era which did not have Deterministic wallets) did not use a new address change but sent the change back to the input address. This reveals the change address exactly.\n",
    "\n",
    "Avoiding address reuse is an obvious remedy. Another idea is that those wallets could automatically detect when a payment address has been used before (perhaps by asking the user) and then use a reused address as their change address; so both outputs would be reused addresses.\n",
    "\n",
    "Also, most reused addresses are mentioned on the internet, forums, social networks like Facebook, Reddit, Stackoverflow...etc. These addresses you can find and check on https://checkbitcoinaddress.com/ site. It's like a little bit de-anonymization of pseudo-anonymized blockchain.\n",
    "\n",
    "Wallet fingerprinting\n",
    "A careful analyst sometimes deduce which software created a certain transaction, because the many different wallet softwares don't always create transactions in exactly the same way. Wallet fingerprinting can be used to detect change outputs because a change output is the one spent with the same wallet fingerprint.\n",
    "\n",
    "As an example, consider five typical transactions that consume one input each and produce two outputs. A, B, C, D, E refer to transactions. A1, A2, etc refer to output addresses of those transactions\n",
    "\n",
    "           --> C1\n",
    "A1 --> B1  --> C2\n",
    "   --> B2  --> D1\n",
    "           --> D2 --> E1\n",
    "                  --> E2\n",
    "\n",
    "If wallet fingerprinting finds that transactions A, B, D and E are created by the same wallet software, and the other transactions are created by other software, then the change addresses become obvious. The same transactions with non-matching addresses replaced by X is shown. The peel chain is visible, it's clear that B2, D2, E1 are change addresses which belong to the same wallet as A1.\n",
    "\n",
    "           --> X\n",
    "A1 --> X   --> X\n",
    "   --> B2  --> X\n",
    "           --> D2 --> E1\n",
    "                  --> X\n",
    "There are a number of ways to get evidence used for identifying wallet software:\n",
    "\n",
    "Address formats. Wallets generally only use one address type. If a transaction has all inputs and one output of the same address type (e.g. p2pkh), with the remaining output of a different type (p2sh), then a reasonable assumption is that the same-address-format output (p2pkh) is change and the different-address-format output (p2sh) is the payment which belongs to someone else.\n",
    "Script types. Each wallet generally uses only one script. For example, the sending wallet may be a P2SH 2-of-3 multisignature wallet, which makes a transaction to two outputs: one 2-of-3 multisignature address and the other 2-of-2 multisignature address. The different script is a strong indication that the output is payment and the other output is change.\n",
    "BIP69 Lexicographical Indexing of Transaction Inputs and Outputs. This BIP describes a standard way for wallets to order their inputs and outputs for privacy. Right now the wallet ecosystem has a mixture of wallets which do and don't implement the standard, which helps with fingerprinting. Note that the common one-input-two-output transaction with random ordering will follow BIP69 just by chance 50% of the time.\n",
    "Number of inputs and outputs. Different users often construct transactions differently. For example, individuals often make transaction with just two outputs; a payment and change, while high-volume institutions like casinos or exchanges use consolidation and batching[6][7]. An output that is later use to create a batching transaction was probably not the change. This heuristic is also called the \"consumer heuristic\".\n",
    "Transaction fields. Values in the transaction format which may vary depending on the wallet software: nLockTime is a field in transactions set by some wallets to make fee sniping less profitable. A mixture of wallets in the ecosystem do and don't implement this feature. nLockTime can also be used as in certain privacy protocols like CoinSwap. nSequence is another example. Also the version number.\n",
    "Low-R value signatures. The DER format used to encode Bitcoin signatures requires adding an entire extra byte to a signature just to indicate when the signature’s R value is on the top-half of the elliptical curve used for Bitcoin. The R value is randomly derived, so half of all signatures have this extra byte. As of July 2018[8]Bitcoin Core only generates signatures with a low-R value that don't require this extra byte. By doing so, Bitcoin Core transactions will save one byte per every two signatures (on average). As of 2019 no other wallet does this, so a high-R signature is evidence that Bitcoin Core is not being used[9].\n",
    "Uncompressed and compressed public keys. Older wallet software uses uncompressed public keys[10]. A mixture of compressed and uncompressed keys can be used for fingerprinting.\n",
    "Miner fees. Various wallet softwares may respond to block space pressure in different ways which could lead to different kinds of miner fees being paid. This might also be a way of fingerprinting wallets.\n",
    "Coin Selection. Various wallet softwares may choose which UTXOs to spend using different algorithms that could be used for fingerprinting.\n",
    "If multiple users are using the same wallet software, then wallet fingerprinting cannot detect the change address. It is also possible that a single user owns two different wallets which use different software (for example a hot wallet and cold wallet) and then transactions between different softwares would not indicate a change of ownership. Wallet fingerprinting on its own is never decisive evidence, but as with all other privacy leaks it works best with data fusion when multiple privacy leaks are combined.\n",
    "\n",
    "Round numbers\n",
    "Many payment amounts are round numbers, for example 1 BTC or 0.1 BTC. The leftover change amount would then be a non-round number (e.g. 1.78213974 BTC). This potentially useful for finding the change address. The amount may be a round number in another currency. The amount 2.24159873 BTC isn't round in bitcoin but when converted to USD it may be close to $100.\n",
    "\n",
    "Fee bumping\n",
    "BIP 0125 defines a mechanism for replacing an unconfirmed transaction with another transaction that pays a higher fee. In the context of the market for block space, a user may find their transaction isn't confirming fast enough so they opt to \"fee bump\" or pay a higher miner fee. However generally the new higher miner fee will happen by reducing the change amount. So if an adversary is observing all unconfirmed transactions they could see both the earlier low-fee transaction and later high-fee transaction, and the output with the reduced amount would be the change output.\n",
    "\n",
    "This could be mitigated by some of the time reducing the amount of both outputs, reducing the payment amount instead of change (in a receiver-pays-for-fee model), or replacing both addresses in each RBF transaction (this would require obtaining multiple payment addresses from the receiver).\n",
    "\n",
    "Unnecessary input heuristic\n",
    "Also called the \"optimal change heuristic\". Consider this bitcoin transaction. It has two inputs worth 2 BTC and 3 BTC and two outputs worth 4 BTC and 1 BTC.\n",
    "\n",
    "    2 btc --> 4 btc\n",
    "    3 btc     1 btc\n",
    "Assuming one of the outputs is change and the other output is the payment. There are two interpretations: the payment output is either the 4 BTC output or the 1 BTC output. But if the 1 BTC output is the payment amount then the 3 BTC input is unnecessary, as the wallet could have spent only the 2 BTC input and paid lower miner fees for doing so. This is an indication that the real payment output is 4 BTC and that 1 BTC is the change output.\n",
    "\n",
    "This is an issue for transactions which have more than one input. One way to fix this leak is to add more inputs until the change output is higher than any input, for example:\n",
    "\n",
    "    2 btc --> 4 btc\n",
    "    3 btc     6 btc\n",
    "    5 btc\n",
    "Now both interpretations imply that some inputs are unnecessary. Unfortunately this costs more in miner fees and can only be done if the wallet actually owns other UTXOs.\n",
    "\n",
    "Some wallets have a coin selection algorithm which violates this heuristic. An example might be because the wallets want to consolidate inputs in times of cheap miner fees. So this heuristic is not decisive evidence.\n",
    "\n",
    "Sending to a different script type\n",
    "Sending funds to a different script type than the one you're spending from makes it easier to tell which output is the change.\n",
    "\n",
    "For example, for a transaction with 1 input spending a p2pkh coin and creating 2 outputs, one of p2pkh and one of p2sh, it is very likely that the p2pkh output is the change while the p2sh one is the payment.\n",
    "\n",
    "This is also possible if the inputs are of mixed types (created by wallets supporting multiple script types for backwards compatibility). If one of the output script types is known to be used by the wallet (because the same script type is spent by at least one of the inputs) while the other is not, the other one is likely to be the payment.\n",
    "\n",
    "This has the most effect on early adopters of new wallet technology, like p2sh or segwit. The more rare it is to pay to people using the same script type as you do, the more you leak the identity of your change output. This will improve over time as the new technology gains wider adoption.\n",
    "\n",
    "Wallet bugs\n",
    "Some wallet software handles change in a very un-private way. For example certain old wallets would always put the change output in last place in the transaction. An old version of Bitcoin Core would add input UTXOs to the transaction until the change amount was around 0.1 BTC, so an amount of slightly over 0.1 BTC would always be change.\n",
    "\n",
    "Equal-output CoinJoin\n",
    "Equal-output-CoinJoin transactions trivially reveal the change address because it is the outputs which are not equal-valued. For example consider this equal-output-coinjoin:\n",
    "\n",
    "                  A (1btc)\n",
    "    X (5btc) ---> B (1btc)\n",
    "    Y (3btc)      C (4btc)\n",
    "                  D (2btc)\n",
    "There is a very strong indication that output D is change belongs to the owner of input Y, while output C is change belonging to input X. However, CoinJoin breaks the common-input-ownership heuristic and effectively hides the ownership of payment outputs (A and B), so the tradeoffs are still heavily in favour of using coinjoin.\n",
    "\n",
    "Cluster growth\n",
    "Wallet clusters created by using the common-input-ownership heuristic usually grow (in number of addresses) slowly and incrementally. Two large clusters merging is rare and may indicate that the heuristics are flawed. So another way to deduce the change address is to find which output causes the clusters to grow only slowly. The exact value for \"how slowly\" a cluster is allowed to grow is an open question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ljNzsN4jOkKl"
   },
   "source": [
    "### **4.5.3.** Bitcoin Address Clustering Algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fyu6IltbUwTr"
   },
   "outputs": [],
   "source": [
    "def input_addresses(G, tx_id):\n",
    "    s = set()\n",
    "    for src, dst in G.in_edges(tx_id):\n",
    "        for arrow_info in G[src][dst].values():\n",
    "            s.add(arrow_info['address'])\n",
    "    return s\n",
    "\n",
    "def output_addresses(G, tx_id):\n",
    "    s = set()\n",
    "    for src, dst in G.out_edges(tx_id):\n",
    "        for arrow_info in G[src][dst].values():\n",
    "            s.add(arrow_info['address'])\n",
    "    return s\n",
    "\n",
    "def in_degree_tx(G, tx_id):\n",
    "    return len(input_addresses(G, tx_id))\n",
    "\n",
    "def out_degree_tx(G, tx_id):\n",
    "    return len(output_addresses(G, tx_id))\n",
    "\n",
    "def is_coinbase_tx(G, tx_id):\n",
    "  for src, dst in G.in_edges(tx_id):\n",
    "    if src[:5] == 'coinb':\n",
    "      return True\n",
    "  return False\n",
    "\n",
    "def is_mined_by_satoshi(G, tx_id, nodeDict, addressDict, start_block):\n",
    "    return is_coinbase_tx(G, tx_id) and start_block == 0 and nodeDict[tx_id]['block_height'] < 21000 and len({addr for addr in output_addresses(G, tx_id) if addressDict[addr]['utilization'] <= 2}) == len(output_addresses(G, tx_id))\n",
    "\n",
    "def is_mined_by_miner(G, tx_id):\n",
    "    return is_coinbase_tx(G, tx_id)\n",
    "\n",
    "def is_consolidation_tx(G, tx_id):\n",
    "    return out_degree_tx(G, tx_id) == 1\n",
    "\n",
    "def is_payment_tx(G, tx_id):\n",
    "    return out_degree_tx(G, tx_id) == 2\n",
    "\n",
    "def is_tx_with_change(G, tx_id, addr_clust):\n",
    "    return not is_coinbase_tx(G, tx_id) and not is_coinjoin_tx(G, tx_id, addr_clust) and out_degree_tx(G, tx_id) > 1\n",
    "\n",
    "def is_coinjoin_tx(G, tx_id, addr_clust):\n",
    "    if not is_coinbase_tx(G, tx_id):\n",
    "        in_addrs = input_addresses(G, tx_id)\n",
    "        out_addrs = output_addresses(G, tx_id)\n",
    "        in_cluster_set = {addr_clust[addr] for addr in in_addrs}\n",
    "        return len(in_cluster_set) == len(in_addrs) and len(out_addrs) >= 10\n",
    "    return False\n",
    "\n",
    "def taint_analysis(G, tx_id):\n",
    "    input = set()\n",
    "    for src, dst in G.in_edges(tx_id):\n",
    "        for arrow_info in G[src][dst].values():\n",
    "            input.add((arrow_info['address'], arrow_info['value']))\n",
    "    output = set()\n",
    "    for src, dst in G.out_edges(tx_id):\n",
    "        for arrow_info in G[src][dst].values():\n",
    "            output.add((arrow_info['address'], arrow_info['value']))\n",
    "    if len(input) > 1:\n",
    "        first_max = builtins.max(input, key=lambda x: x[1])\n",
    "        input.remove(first_max)\n",
    "        second_max = builtins.max(input, key=lambda x: x[1])\n",
    "        possible_address = {tup[0] for tup in output if tup[1] > second_max[1] and tup[1] < first_max[1]}\n",
    "        if len(possible_address) == 1:\n",
    "            return [{possible_address.pop(), first_max[0]}]\n",
    "    return []\n",
    "\n",
    "def find_change_address(G, tx_id, nodeDict, addressDict, addr_clust, clust_addr):\n",
    "    \n",
    "    input = set()\n",
    "    for src, dst in G.in_edges(tx_id):\n",
    "        for arrow_info in G[src][dst].values():\n",
    "            input.add((arrow_info['address'], arrow_info['value']))\n",
    "    output = set()\n",
    "    for src, dst in G.out_edges(tx_id):\n",
    "        for arrow_info in G[src][dst].values():\n",
    "            output.add((arrow_info['address'], arrow_info['value']))\n",
    "\n",
    "    in_cluster_set = {addr_clust[addr] for addr, _ in input}\n",
    "    out_cluster_set = {addr_clust[addr] for addr, _ in output}\n",
    "    intersection_cluster_set = in_cluster_set.intersection(out_cluster_set)\n",
    "\n",
    "    if intersection_cluster_set != set():\n",
    "        for cluster_id in intersection_cluster_set:\n",
    "            for addr in clust_addr[cluster_id]:\n",
    "                for a, _ in output:\n",
    "                    if addr == a:\n",
    "                        return addr\n",
    "\n",
    "    else:\n",
    "        possible_change_addresses = set()\n",
    "\n",
    "        # Unnecessary input heuristic\n",
    "        for out in output:\n",
    "            flag = False\n",
    "            for inp in input:\n",
    "                if out[1] < inp[1]:\n",
    "                    flag = True\n",
    "                    break\n",
    "            if flag:\n",
    "                possible_change_addresses.add(out[0])\n",
    "        if len(possible_change_addresses) == 1:\n",
    "            return possible_change_addresses.pop()\n",
    "        \n",
    "        # new address in output heuristic\n",
    "        new_addrs = {addr for addr, _ in output if nodeDict[tx_id]['temporal_index'] == addressDict[addr]['birthday']}\n",
    "        if len(new_addrs) == 1:\n",
    "            return new_addrs.pop()\n",
    "        else:\n",
    "            possible_change_addresses = possible_change_addresses.union(new_addrs)\n",
    "\n",
    "        # round number heuristic\n",
    "        if len(output) == 2:\n",
    "            o1, o2 = output.pop(), output.pop()\n",
    "            s1 = str(o1[1]).strip('0').strip('.')\n",
    "            s2 = str(o1[1]).strip('0').strip('.')\n",
    "            l1 = len(s1.split('.')[1]) if '.' in s1 else 0 \n",
    "            l2 = len(s2.split('.')[1]) if '.' in s2 else 0 \n",
    "            if l1-3 > l2:\n",
    "                return o1[0]\n",
    "            if l2-3 > l1:\n",
    "                return o2[0]\n",
    "        \n",
    "        possible_change_addresses = {addr for addr in possible_change_addresses if addressDict[addr]['utilization'] <= 2}\n",
    "        if len(possible_change_addresses) == 1:\n",
    "            return possible_change_addresses.pop()\n",
    "    return None\n",
    "\n",
    "def apply_hard_heuristics(G, tx_id, nodeDict, addressDict, addr_clust, clust_addr):\n",
    "    if is_tx_with_change(G, tx_id, addr_clust):\n",
    "        change_address = find_change_address(G, tx_id, nodeDict, addressDict, addr_clust, clust_addr)\n",
    "        if change_address != None:\n",
    "            return [input_addresses(G, tx_id).union({change_address})]\n",
    "        else:\n",
    "            return [input_addresses(G, tx_id)]\n",
    "    if is_coinjoin_tx(G, tx_id, addr_clust):\n",
    "        return taint_analysis(G, tx_id)\n",
    "    return []\n",
    "\n",
    "def apply_simple_heuristics(G, tx_id, start_block, nodeDict, addressDict):\n",
    "    if is_mined_by_satoshi(G, tx_id, nodeDict, addressDict, start_block):\n",
    "        # Satoshi heuristic\n",
    "        return [output_addresses(G, tx_id).union({'1A1zP1eP5QGefi2DMPTfTL5SLmv7DivfNa'})]\n",
    "    if is_mined_by_miner(G, tx_id): \n",
    "        # Coinbase transaction mining address clustering heuristic\n",
    "        return [output_addresses(G, tx_id)]\n",
    "    if is_consolidation_tx(G, tx_id):\n",
    "        # Consolidation transaction heuristic\n",
    "        return [input_addresses(G, tx_id).union(output_addresses(G, tx_id))]\n",
    "    if is_payment_tx(G, tx_id):\n",
    "        # Payment transaction with amount payed and change address\n",
    "        return [input_addresses(G, tx_id)]\n",
    "    return []\n",
    "\n",
    "def merge_clusters(list_of_grouped_addresses_sets, addr_clust, clust_addr):\n",
    "    for grouped_addresses_set in list_of_grouped_addresses_sets:\n",
    "        #ogni set grouped_addresses_set ha almeno due address\n",
    "        cluster_id_to_merge = set(addr_clust[address] for address in grouped_addresses_set)\n",
    "        common_cluster_id = cluster_id_to_merge.pop()\n",
    "        \n",
    "        for cluster_id in cluster_id_to_merge:\n",
    "            clust_addr[common_cluster_id] = clust_addr[common_cluster_id].union(clust_addr[cluster_id])\n",
    "\n",
    "            for address in clust_addr[cluster_id]:\n",
    "                addr_clust[address] = common_cluster_id\n",
    "            \n",
    "            del clust_addr[cluster_id]\n",
    "    return\n",
    "\n",
    "def address_clustering(G, a_df, known_tx_df, spark_session, start_block, debug=False):\n",
    "    \n",
    "    if debug: print('initialing for clustering...')\n",
    "    addr_clust = {row.address : row.cluster_id for row in a_df.rdd.collect()}\n",
    "    clust_addr = {row.cluster_id : {row.address} for row in a_df.rdd.collect()}\n",
    "    list_of_grouped_addresses_sets = []\n",
    "    addressDict = dict()\n",
    "    nodeDict = dict()\n",
    "    \n",
    "    for i, tx in known_tx_df.sort('temporal_index').toPandas().iterrows():\n",
    "        nodeDict[str(tx.id)] = {'temporal_index' : tx.temporal_index, 'block_height' : tx.block_height} \n",
    "        for address in output_addresses(G, str(tx.id)).union(input_addresses(G, str(tx.id))):\n",
    "            try:\n",
    "                addressDict[address]['utilization'] += 1\n",
    "            except KeyError:\n",
    "                addressDict[address] = {'birthday' : tx.temporal_index, 'utilization' : 1}\n",
    "    if debug: print('number of clusters at starting point: {}'.format(len(clust_addr)))\n",
    "\n",
    "    if debug: print('apply simple heuristics...')\n",
    "    for i, tx in known_tx_df.sort('temporal_index').toPandas().iterrows():\n",
    "        if debug and i % 100000 == 0: print('clustering tx number {}'.format(i))\n",
    "        list_of_grouped_addresses_sets = apply_simple_heuristics(G, str(tx.id), start_block, nodeDict, addressDict)\n",
    "        merge_clusters(list_of_grouped_addresses_sets, addr_clust, clust_addr)\n",
    "    \n",
    "    if debug: print('number of clusters after simple heuristic: {}'.format(len(clust_addr)))\n",
    "\n",
    "    if debug: print('apply hard heuristics...')\n",
    "    for i, tx in known_tx_df.sort('temporal_index').toPandas().iterrows():\n",
    "        if debug and i % 100000 == 0: print('clustering tx number {}'.format(i))\n",
    "        list_of_grouped_addresses_sets = apply_hard_heuristics(G, str(tx.id), nodeDict, addressDict, addr_clust, clust_addr)\n",
    "        merge_clusters(list_of_grouped_addresses_sets, addr_clust, clust_addr)\n",
    "    \n",
    "    if debug: print('number of clusters after hard heuristic: {}'.format(len(clust_addr)))\n",
    "\n",
    "    a_df = spark_session.createDataFrame(addr_clust.items(), ['address', 'cluster_id'])\n",
    "    return a_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WfJIiVnR7-11"
   },
   "outputs": [],
   "source": [
    "a_df = address_clustering(nx_graph, a_df, known_tx_df, spark, start_block, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XatCitOBLEcX"
   },
   "outputs": [],
   "source": [
    "shutil.rmtree(os.path.join(d_path, 'addresses-{}-{}'.format(start_block, end_block)))\n",
    "a_df.write.save(path=a_path, format='csv', header='True')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w5cH-NJrTdUW"
   },
   "source": [
    "# **5. Results**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sDosNsJOz_bt"
   },
   "source": [
    "## **5.1.** Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hd1OlHTDNVhL"
   },
   "outputs": [],
   "source": [
    "a_df.createOrReplaceTempView('ADDRESS')\n",
    "entity_df = spark.sql(\"select min(address) as representative, cluster_id as entity_id, count(cluster_id) as n_address_collected from ADDRESS group by cluster_id order by -count(cluster_id)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TsXql8PWOMVZ"
   },
   "outputs": [],
   "source": [
    "print(entity_df.count())\n",
    "entity_df.printSchema()\n",
    "entity_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dwxg0sWtO7_4"
   },
   "outputs": [],
   "source": [
    "entity_df.createOrReplaceTempView('ENTITY')\n",
    "statistics_entity_df = spark.sql(\"select n_address_collected as entity_dimention, count(n_address_collected) as n_entities from ENTITY group by n_address_collected order by n_address_collected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g1P9JbDkPz64"
   },
   "outputs": [],
   "source": [
    "print(statistics_entity_df.count())\n",
    "statistics_entity_df.printSchema()\n",
    "statistics_entity_df.show(statistics_entity_df.count(), truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-yAbyEkcCL5V"
   },
   "outputs": [],
   "source": [
    "plt.scatter(\n",
    "    x = statistics_entity_df.toPandas()['entity_dimention'], \n",
    "    y = statistics_entity_df.toPandas()['entity_dimention'], \n",
    "    s=statistics_entity_df.toPandas()['n_entities'], \n",
    "    alpha=0.5)\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.xlabel(\"entity_dimention\")\n",
    "plt.ylabel(\"entity_dimention\")\n",
    "plt.title(\"Entity dimention distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0J5Hk2iD0GYP"
   },
   "source": [
    "## **5.2.** Assumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ER9qZprh7ibO"
   },
   "outputs": [],
   "source": [
    "entity_df.createOrReplaceTempView('ENTITY')\n",
    "tmp_df = spark.sql(\"select entity_id from ENTITY where n_address_collected > 1\")\n",
    "a2_df = a_df.join(tmp_df, a_df.cluster_id == tmp_df.entity_id, 'inner').select('address', 'cluster_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sM0MGkOX-iMG"
   },
   "outputs": [],
   "source": [
    "print(a2_df.count())\n",
    "a2_df.printSchema()\n",
    "a2_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dp0C2f-oBrsz"
   },
   "source": [
    "## **5.3.** Overview after Assumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L4cOQZg9B6Cy"
   },
   "outputs": [],
   "source": [
    "a2_df.createOrReplaceTempView('ADDRESS')\n",
    "entity_df = spark.sql(\"select min(address) as representative, cluster_id as entity_id, count(cluster_id) as n_address_collected from ADDRESS group by cluster_id order by -count(cluster_id)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vy4Vk-w5CAHO"
   },
   "outputs": [],
   "source": [
    "print(entity_df.count())\n",
    "entity_df.printSchema()\n",
    "entity_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OIJvBJmFC9Sp"
   },
   "outputs": [],
   "source": [
    "entity_df.createOrReplaceTempView('ENTITY')\n",
    "statistics_entity_df = spark.sql(\"select n_address_collected as entity_dimention, count(n_address_collected) as n_entities from ENTITY group by n_address_collected order by n_address_collected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hFp2rTfvDBw1"
   },
   "outputs": [],
   "source": [
    "print(statistics_entity_df.count())\n",
    "statistics_entity_df.printSchema()\n",
    "statistics_entity_df.show(statistics_entity_df.count(), truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QWtpV8u1ByDu"
   },
   "outputs": [],
   "source": [
    "plt.scatter(\n",
    "    x = statistics_entity_df.toPandas()['entity_dimention'], \n",
    "    y = statistics_entity_df.toPandas()['entity_dimention'], \n",
    "    s=statistics_entity_df.toPandas()['n_entities'], \n",
    "    alpha=0.5)\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.xlabel(\"entity_dimention\")\n",
    "plt.ylabel(\"entity_dimention\")\n",
    "plt.title(\"Entity dimention distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eyosgUzv4oj_"
   },
   "source": [
    "# **6. Boosting Bitcoin Address Clustering with other more complex techniques**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "glg6XYCN45P1"
   },
   "source": [
    "## **6.1.** Merge or Demerge clusters thanks to off-chain informations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "00bgUI736EM6"
   },
   "outputs": [],
   "source": [
    "satoshi = {'1A1zP1eP5QGefi2DMPTfTL5SLmv7DivfNa', '12cbQLTFMXRnSzktFkuoG3eHoMeFtpTu3S'}\n",
    "lazslo = {'1Q2TWHE3GMdB6BZKafqwxXtWAWgFt5Jvm3', '1XPTgDRhN8RFnzniWCddobD9iKZatrvH4'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "edtS-k7Z5hPI"
   },
   "source": [
    "## **6.2.** Louvain community detection algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XrziQyl8Tolv"
   },
   "source": [
    "# **7. Use cases**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4D9RXQaViain"
   },
   "source": [
    "## **7.1.** Visualize entity movements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l5IYUnR0_JBQ",
    "outputId": "f4803c0b-91f7-4d2e-dc83-9af82e3d4d36"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "address 12cbQLTFMXRnSzktFkuoG3eHoMeFtpTu3S is in cluster 51539615536\n"
     ]
    }
   ],
   "source": [
    "address = '12cbQLTFMXRnSzktFkuoG3eHoMeFtpTu3S'\n",
    "ClusterList = None\n",
    "try:\n",
    "    a_df.createOrReplaceTempView('ADDRESSES')\n",
    "    cluster_id = spark.sql(\"select cluster_id from ADDRESSES where address = '\"+address+\"'\").head().cluster_id\n",
    "    a_df.createOrReplaceTempView('ADDRESSES')\n",
    "    ClusterList = spark.sql(\"select address from ADDRESSES where cluster_id = \" + str(cluster_id)).withColumnRenamed('address', 'address_clustered')\n",
    "    print('address {} is in cluster {}'.format(address, cluster_id))\n",
    "except:\n",
    "    print('invalid address')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HxeKPPUPcEET"
   },
   "outputs": [],
   "source": [
    "new_e_df = e_df.join(ClusterList, e_df.address == ClusterList.address_clustered, 'inner').drop('address_clustered')\n",
    "tmp_df = new_e_df.select(new_e_df.src_id).union(new_e_df.select(new_e_df.dst_id)).withColumnRenamed('src_id', 'tmp_id').distinct()\n",
    "new_v_df = v_df.join(tmp_df, v_df.id == tmp_df.tmp_id, 'inner').drop('tmp_id')\n",
    "tmp_df = new_v_df.select('id')\n",
    "new_e_df = e_df.join(tmp_df, tmp_df.id == e_df.src_id, 'inner').union(e_df.join(tmp_df, tmp_df.id == e_df.dst_id, 'inner')).drop('id').distinct()\n",
    "tmp_df = new_e_df.select(new_e_df.src_id).union(new_e_df.select(new_e_df.dst_id)).distinct().withColumnRenamed('src_id', 'tmp_id')\n",
    "new_v_df = v_df.join(tmp_df, v_df.id == tmp_df.tmp_id).drop('tmp_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kBv7_P5eUQBd",
    "outputId": "860cf909-96aa-4376-cd81-b0462e50303a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total nodes: 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total edges: 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(\"Total nodes: {}\".format(new_v_df.count()))\n",
    "print(\"Total edges: {}\".format(new_e_df.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xC2FegsFiHR5",
    "outputId": "2da91385-39d0-4894-df01-432cbd8e79ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------+----------------------------------------------------------------+------------+----------------------------------------------------------------+---+-------+------------+--------+-------------+--------------+\n",
      "|id              |note    |tx_hash                                                         |block_height|block_hash                                                      |fee|n_input|amount_input|n_output|amount_output|temporal_index|\n",
      "+----------------+--------+----------------------------------------------------------------+------------+----------------------------------------------------------------+---+-------+------------+--------+-------------+--------------+\n",
      "|8938199527718782|tx      |828ef3b079f9c23829c56fe86e85b4a69d9e06e5b54ea597eef5fb3ffef509fe|248         |00000000fb5b44edc7a1aa105075564a179d65506e2bd25f55f1629251d0f6b0|0  |1      |2800000000  |2       |2800000000   |255           |\n",
      "|4285570218664044|tx      |4385fcf8b14497d0659adccfe06ae7e38e0b5dc95ff8a13d7c62035994a0cd79|187         |00000000b2cde2159116889837ecf300bd77d229d49b138c55366b54626e495d|0  |1      |100000000   |1       |100000000    |192           |\n",
      "|5011701965475923|tx      |ea44e97271691990157559d0bdd9959e02790c34db6c006d779e82fa5aee708e|92240       |0000000000077430a94a5376bf2af42d4b1aebdecedfa9e4f7e3f0465a84d891|0  |1      |1000000000  |1       |1000000000   |156741        |\n",
      "|UTXO248         |UTXO    |null                                                            |-1          |null                                                            |0  |1      |1800000000  |0       |0            |-1            |\n",
      "|6564325921434451|tx      |12b5633bad1f9c167d523ad1aa1947b2732a865bf5414eab2f9e5ae5d5c191ba|183         |00000000f46e513f038baf6f2d9a95b2a28d8a6c985bcf24b9e07f0f63a29888|0  |1      |2900000000  |2       |2900000000   |187           |\n",
      "|6687795960110968|tx      |a16f3ce4dd5deb92d98ef5cf8afeaf0775ebca408f708b2146c4fb42b41e14be|181         |00000000dc55860c8a29c58d45209318fa9e9dc2c1833a7226d86bc465afc6e5|0  |1      |4000000000  |2       |4000000000   |183           |\n",
      "|7092901136679432|tx      |0437cd7f8525ceed2324359c2d0ba26006d92d856a9c20fa0241106ee5a597c9|9           |000000008d9dc510f23c2657fc4f67bea30078cc05a90eb89e84cc475c080805|0  |1      |5000000000  |1       |5000000000   |9             |\n",
      "|1816063685936491|tx      |a3b0e9e7cddbbe78270fa4182a7675ff00b92872d8df7d14265a2b1e379a9d33|496         |00000000b0c5a240b2a61d2e75692224efd4cbecdf6eaf4cc2cf477ca7c270e7|0  |3      |6100000000  |1       |6100000000   |504           |\n",
      "|UTXO180         |UTXO    |null                                                            |-1          |null                                                            |0  |1      |1000000000  |0       |0            |-1            |\n",
      "|795787923367440 |tx      |f4184fc596403b9d638783cf57adfe4c75c605f6356fbc91338530e9831e9e16|170         |00000000d1145790a8694403d4063f323d499e655c83426834d4ce2f8dd4a2ee|0  |1      |5000000000  |2       |5000000000   |171           |\n",
      "|coinbase9       |coinbase|null                                                            |-1          |null                                                            |0  |0      |0           |1       |5000000000   |-1            |\n",
      "|3391095309349408|tx      |298ca2045d174f8a158961806ffc4ef96fad02d71a6b84d9fa0491813a776160|221         |0000000066356691a4353dd8bdc2c60da20d68ad34fff93d8839a133b2a6d42a|0  |1      |100000000   |1       |100000000    |227           |\n",
      "|4063842432401470|tx      |591e91f809d716912ca1d4a9295e70c3e78bab077683f79350f101da64588073|182         |0000000054487811fc4ff7a95be738aa5ad9320c394c482b27c0da28b227ad5d|0  |1      |3000000000  |2       |3000000000   |185           |\n",
      "+----------------+--------+----------------------------------------------------------------+------------+----------------------------------------------------------------+---+-------+------------+--------+-------------+--------------+\n",
      "\n",
      "+----------------+----------------+------------+------------+----------------------------------+----------+\n",
      "|src_id          |dst_id          |src_position|dst_position|address                           |value     |\n",
      "+----------------+----------------+------------+------------+----------------------------------+----------+\n",
      "|8938199527718782|UTXO248         |1           |-1          |12cbQLTFMXRnSzktFkuoG3eHoMeFtpTu3S|1800000000|\n",
      "|6687795960110968|4063842432401470|1           |0           |12cbQLTFMXRnSzktFkuoG3eHoMeFtpTu3S|3000000000|\n",
      "|795787923367440 |6687795960110968|1           |0           |12cbQLTFMXRnSzktFkuoG3eHoMeFtpTu3S|4000000000|\n",
      "|6564325921434451|8938199527718782|1           |0           |12cbQLTFMXRnSzktFkuoG3eHoMeFtpTu3S|2800000000|\n",
      "|4063842432401470|3391095309349408|0           |0           |1LzBzVqEeuQyjD2mRWHes3dgWrT9titxvq|100000000 |\n",
      "|7092901136679432|795787923367440 |0           |0           |12cbQLTFMXRnSzktFkuoG3eHoMeFtpTu3S|5000000000|\n",
      "|8938199527718782|1816063685936491|0           |2           |1ByLSV2gLRcuqUmfdYcpPQH8Npm8cccsFg|1000000000|\n",
      "|795787923367440 |5011701965475923|0           |0           |1Q2TWHE3GMdB6BZKafqwxXtWAWgFt5Jvm3|1000000000|\n",
      "|6687795960110968|UTXO180         |0           |-1          |1DUDsfc23Dv9sPMEk5RsrtfzCw5ofi5sVW|1000000000|\n",
      "|4063842432401470|6564325921434451|1           |0           |12cbQLTFMXRnSzktFkuoG3eHoMeFtpTu3S|2900000000|\n",
      "|6564325921434451|4285570218664044|0           |0           |13HtsYzne8xVPdGDnmJX8gHgBZerAfJGEf|100000000 |\n",
      "|coinbase9       |7092901136679432|4294967295  |0           |coinbase9                         |5000000000|\n",
      "+----------------+----------------+------------+------------+----------------------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_v_df.show(truncate=False)\n",
    "new_e_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6IF_8tXPiHJr"
   },
   "outputs": [],
   "source": [
    "def generate_pyvis_graph(v_df, e_df, a_df, cluster_id_selected):\n",
    "\n",
    "    addr_clust = {row.address : row.cluster_id for row in a_df.rdd.collect()}\n",
    "\n",
    "    net = Network(\n",
    "                height='100%',\n",
    "                width='100%',\n",
    "                directed=True\n",
    "                )\n",
    "    net.repulsion(\n",
    "                node_distance=420,\n",
    "                central_gravity=0.33,\n",
    "                spring_length=110,\n",
    "                spring_strength=0.10,\n",
    "                damping=0.95\n",
    "                )\n",
    "\n",
    "    for node in v_df.rdd.collect():\n",
    "        if node.note == 'tx':\n",
    "            title = 'tx_hash: {}\\n'.format(node.tx_hash) + \\\n",
    "                'block_height: {}\\n'.format(node.block_height) + \\\n",
    "                'n_input: {}\\n'.format(node.n_input) + \\\n",
    "                'n_output: {}\\n'.format(node.n_output) + \\\n",
    "                'amount_input: {} BTC\\n'.format(node.amount_input/100000000) + \\\n",
    "                'amount_output: {} BTC\\n'.format(node.amount_output/100000000) + \\\n",
    "                'fee: {} BTC'.format(node.fee/100000000)\n",
    "            net.add_node(n_id=str(node.id), shape='dot', title=title)\n",
    "        elif node.note == 'UTXO':\n",
    "            title = '{}\\n'.format(node.note) + \\\n",
    "                'amount: {} BTC'.format(node.amount_input/100000000)\n",
    "            net.add_node(n_id=str(node.id), shape='square', title=title)\n",
    "        elif node.note == 'coinbase':\n",
    "            title = '{}\\n'.format(node.note) + \\\n",
    "                'amount: {} BTC'.format(node.amount_output/100000000)\n",
    "            net.add_node(n_id=str(node.id), shape='square', title=title)\n",
    "        elif node.note == 'unknown_tx':\n",
    "            title = 'unknown_tx\\n' + 'tx_index: {}'.format(node.id)\n",
    "            net.add_node(n_id=str(node.id), shape='square', title=title)\n",
    "\n",
    "    for edge in e_df.rdd.collect():\n",
    "        if edge.address[:4] != 'UTXO' and edge.address[:4] != 'coin':\n",
    "            title = 'address: {}\\n'.format(edge.address) + \\\n",
    "                    'value: {} BTC\\n'.format(edge.value/100000000) + \\\n",
    "                    'cluster_id : {}'.format(addr_clust[edge.address]) \n",
    "            if addr_clust[edge.address] == cluster_id_selected:\n",
    "                net.add_edge(source=str(edge.src_id), to=str(edge.dst_id), title=title, value=edge.value/100000000, color='red')\n",
    "            else:\n",
    "                net.add_edge(source=str(edge.src_id), to=str(edge.dst_id), title=title, value=edge.value/100000000)\n",
    "        else:\n",
    "            title = 'address: {}\\n'.format(edge.address) + \\\n",
    "                    'value: {} BTC\\n'.format(edge.value/100000000)\n",
    "            net.add_edge(source=str(edge.src_id), to=str(edge.dst_id), title=title, value=edge.value/100000000)\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9yR3nTMCiHBw"
   },
   "outputs": [],
   "source": [
    "new_pyvis_graph = generate_pyvis_graph(new_v_df, new_e_df, a_df, cluster_id)\n",
    "new_pyvis_graph.write_html(os.path.join(d_path, 'cluster_graph-' + address + '-{}-{}.html'.format(start_block, end_block)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2x4FyX7pp16j"
   },
   "source": [
    "## **7.2.** Solve specific Queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZbLlB0-srqD1"
   },
   "source": [
    "### **7.2.1.** How \"Trustless\" Is Bitcoin, Really? ([link](https://www.nytimes.com/2022/06/06/science/bitcoin-nakamoto-blackburn-crypto.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "clUizKAysRlW"
   },
   "outputs": [],
   "source": [
    "entity_df.createOrReplaceTempView('ENTITY')\n",
    "tmp_df = spark.sql(\"select entity_id from ENTITY where n_address_collected > 2\")\n",
    "a3_df = a_df.join(tmp_df, a_df.cluster_id == tmp_df.entity_id, 'inner').select('address', 'cluster_id')\n",
    "\n",
    "e_df.createOrReplaceTempView('EDGES')\n",
    "coinbase_tx_id_df = spark.sql(\"select dst_id as tmp_id from EDGES where src_id like 'coinbase%'\")\n",
    "miners_addrs_df = e_df.join(coinbase_tx_id_df, e_df.src_id == coinbase_tx_id_df.tmp_id).select('address').withColumnRenamed('address', 'miner_address')\n",
    "tmp_df = a3_df.join(miners_addrs_df, a3_df.address == miners_addrs_df.miner_address, 'inner').drop('miner_address')\n",
    "tmp_df.createOrReplaceTempView('TMP')\n",
    "miners_entities_df = spark.sql(\"select min(address) as representative, cluster_id, count(cluster_id) as n_address_collected from TMP group by cluster_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6Phln2siyeMA",
    "outputId": "81047242-b3ba-454f-de7f-e9c042fc767a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {
      "text/plain": [
       "1854"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "miners_entities_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nxbRNlBt281J"
   },
   "source": [
    "### **7.2.2.** Any other Queries you can immagine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gqig5HHBExpN"
   },
   "source": [
    "# **8. Web App**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ag_39DP9Dxpx",
    "outputId": "3420e91b-ce91-4459-d55a-89c706f5629c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: traitlets in /usr/local/lib/python3.7/dist-packages (5.1.1)\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: tornado in /usr/local/lib/python3.7/dist-packages (5.1.1)\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting streamlit\n",
      "  Downloading streamlit-1.10.0-py2.py3-none-any.whl (9.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 9.1 MB 5.5 MB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from streamlit) (1.21.6)\n",
      "Requirement already satisfied: importlib-metadata>=1.4 in /usr/local/lib/python3.7/dist-packages (from streamlit) (4.11.4)\n",
      "Requirement already satisfied: tzlocal in /usr/local/lib/python3.7/dist-packages (from streamlit) (1.5.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (7.1.2)\n",
      "Requirement already satisfied: attrs in /usr/local/lib/python3.7/dist-packages (from streamlit) (21.4.0)\n",
      "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (7.1.2)\n",
      "Requirement already satisfied: tornado>=5.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (5.1.1)\n",
      "Collecting gitpython!=3.1.19\n",
      "  Downloading GitPython-3.1.27-py3-none-any.whl (181 kB)\n",
      "\u001b[K     |████████████████████████████████| 181 kB 37.2 MB/s \n",
      "\u001b[?25hCollecting pydeck>=0.1.dev5\n",
      "  Downloading pydeck-0.7.1-py2.py3-none-any.whl (4.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.3 MB 36.2 MB/s \n",
      "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from streamlit) (21.3)\n",
      "Requirement already satisfied: pyarrow in /usr/local/lib/python3.7/dist-packages (from streamlit) (6.0.1)\n",
      "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from streamlit) (2.8.2)\n",
      "Requirement already satisfied: pandas>=0.21.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (1.3.5)\n",
      "Requirement already satisfied: cachetools>=4.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (4.2.4)\n",
      "Collecting toml\n",
      "  Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: altair>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (4.2.0)\n",
      "Collecting rich\n",
      "  Downloading rich-12.4.4-py3-none-any.whl (232 kB)\n",
      "\u001b[K     |████████████████████████████████| 232 kB 21.5 MB/s \n",
      "\u001b[?25hCollecting pympler>=0.9\n",
      "  Downloading Pympler-1.0.1-py3-none-any.whl (164 kB)\n",
      "\u001b[K     |████████████████████████████████| 164 kB 46.6 MB/s \n",
      "\u001b[?25hCollecting watchdog\n",
      "  Downloading watchdog-2.1.9-py3-none-manylinux2014_x86_64.whl (78 kB)\n",
      "\u001b[K     |████████████████████████████████| 78 kB 3.1 MB/s \n",
      "\u001b[?25hRequirement already satisfied: semver in /usr/local/lib/python3.7/dist-packages (from streamlit) (2.13.0)\n",
      "Requirement already satisfied: protobuf<4,>=3.12 in /usr/local/lib/python3.7/dist-packages (from streamlit) (3.17.3)\n",
      "Collecting blinker\n",
      "  Downloading blinker-1.4.tar.gz (111 kB)\n",
      "\u001b[K     |████████████████████████████████| 111 kB 46.8 MB/s \n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from streamlit) (2.23.0)\n",
      "Collecting validators\n",
      "  Downloading validators-0.20.0.tar.gz (30 kB)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from streamlit) (4.1.1)\n",
      "Requirement already satisfied: toolz in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit) (0.11.2)\n",
      "Requirement already satisfied: entrypoints in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit) (0.4)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit) (2.11.3)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit) (4.3.3)\n",
      "Collecting gitdb<5,>=4.0.1\n",
      "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
      "\u001b[K     |████████████████████████████████| 63 kB 1.6 MB/s \n",
      "\u001b[?25hCollecting smmap<6,>=3.0.1\n",
      "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=1.4->streamlit) (3.8.0)\n",
      "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=3.0->altair>=3.2.0->streamlit) (5.7.1)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=3.0->altair>=3.2.0->streamlit) (0.18.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.21.0->streamlit) (2022.1)\n",
      "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf<4,>=3.12->streamlit) (1.15.0)\n",
      "Requirement already satisfied: traitlets>=4.3.2 in /usr/local/lib/python3.7/dist-packages (from pydeck>=0.1.dev5->streamlit) (5.1.1)\n",
      "Collecting ipykernel>=5.1.2\n",
      "  Downloading ipykernel-6.15.0-py3-none-any.whl (133 kB)\n",
      "\u001b[K     |████████████████████████████████| 133 kB 53.8 MB/s \n",
      "\u001b[?25hRequirement already satisfied: ipywidgets>=7.0.0 in /usr/local/lib/python3.7/dist-packages (from pydeck>=0.1.dev5->streamlit) (7.7.0)\n",
      "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (1.5.5)\n",
      "Collecting tornado>=5.0\n",
      "  Downloading tornado-6.1-cp37-cp37m-manylinux2010_x86_64.whl (428 kB)\n",
      "\u001b[K     |████████████████████████████████| 428 kB 46.8 MB/s \n",
      "\u001b[?25hCollecting jupyter-client>=6.1.12\n",
      "  Downloading jupyter_client-7.3.4-py3-none-any.whl (132 kB)\n",
      "\u001b[K     |████████████████████████████████| 132 kB 56.5 MB/s \n",
      "\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (5.4.8)\n",
      "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (23.1.0)\n",
      "Requirement already satisfied: debugpy>=1.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (1.0.0)\n",
      "Collecting ipython>=7.23.1\n",
      "  Downloading ipython-7.34.0-py3-none-any.whl (793 kB)\n",
      "\u001b[K     |████████████████████████████████| 793 kB 54.1 MB/s \n",
      "\u001b[?25hRequirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (0.1.3)\n",
      "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (0.18.1)\n",
      "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (2.6.1)\n",
      "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (0.7.5)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (4.4.2)\n",
      "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (4.8.0)\n",
      "Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (0.2.0)\n",
      "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (57.4.0)\n",
      "Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0\n",
      "  Downloading prompt_toolkit-3.0.29-py3-none-any.whl (381 kB)\n",
      "\u001b[K     |████████████████████████████████| 381 kB 53.8 MB/s \n",
      "\u001b[?25hRequirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (5.4.0)\n",
      "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (1.1.0)\n",
      "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (3.6.0)\n",
      "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.2.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (0.8.3)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->altair>=3.2.0->streamlit) (2.0.1)\n",
      "Requirement already satisfied: jupyter-core>=4.9.2 in /usr/local/lib/python3.7/dist-packages (from jupyter-client>=6.1.12->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (4.10.0)\n",
      "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (2.15.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (0.2.5)\n",
      "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.7/dist-packages (from widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (5.3.1)\n",
      "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.13.3)\n",
      "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (1.8.0)\n",
      "Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (5.6.1)\n",
      "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.6.0)\n",
      "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (5.0.0)\n",
      "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.7.1)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (1.5.0)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.8.4)\n",
      "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.5.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->streamlit) (3.0.9)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->streamlit) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->streamlit) (2022.6.15)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->streamlit) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->streamlit) (2.10)\n",
      "Collecting commonmark<0.10.0,>=0.9.0\n",
      "  Downloading commonmark-0.9.1-py2.py3-none-any.whl (51 kB)\n",
      "\u001b[K     |████████████████████████████████| 51 kB 5.7 MB/s \n",
      "\u001b[?25hBuilding wheels for collected packages: blinker, validators\n",
      "  Building wheel for blinker (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for blinker: filename=blinker-1.4-py3-none-any.whl size=13478 sha256=1c8a771a7c585bd064ca6830974cf958fa35afc40163caebd529d5ae8939b9c0\n",
      "  Stored in directory: /root/.cache/pip/wheels/22/f5/18/df711b66eb25b21325c132757d4314db9ac5e8dabeaf196eab\n",
      "  Building wheel for validators (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for validators: filename=validators-0.20.0-py3-none-any.whl size=19582 sha256=952e72cafaf1e909265aa2035eb947d20d8ff6d7cb25378831704e802d6b8857\n",
      "  Stored in directory: /root/.cache/pip/wheels/5f/55/ab/36a76989f7f88d9ca7b1f68da6d94252bb6a8d6ad4f18e04e9\n",
      "Successfully built blinker validators\n",
      "Installing collected packages: tornado, prompt-toolkit, jupyter-client, ipython, ipykernel, smmap, gitdb, commonmark, watchdog, validators, toml, rich, pympler, pydeck, gitpython, blinker, streamlit\n",
      "  Attempting uninstall: tornado\n",
      "    Found existing installation: tornado 5.1.1\n",
      "    Uninstalling tornado-5.1.1:\n",
      "      Successfully uninstalled tornado-5.1.1\n",
      "  Attempting uninstall: prompt-toolkit\n",
      "    Found existing installation: prompt-toolkit 1.0.18\n",
      "    Uninstalling prompt-toolkit-1.0.18:\n",
      "      Successfully uninstalled prompt-toolkit-1.0.18\n",
      "  Attempting uninstall: jupyter-client\n",
      "    Found existing installation: jupyter-client 5.3.5\n",
      "    Uninstalling jupyter-client-5.3.5:\n",
      "      Successfully uninstalled jupyter-client-5.3.5\n",
      "  Attempting uninstall: ipython\n",
      "    Found existing installation: ipython 5.5.0\n",
      "    Uninstalling ipython-5.5.0:\n",
      "      Successfully uninstalled ipython-5.5.0\n",
      "  Attempting uninstall: ipykernel\n",
      "    Found existing installation: ipykernel 4.10.1\n",
      "    Uninstalling ipykernel-4.10.1:\n",
      "      Successfully uninstalled ipykernel-4.10.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "nbclient 0.6.4 requires traitlets>=5.2.2, but you have traitlets 5.1.1 which is incompatible.\n",
      "jupyter-console 5.2.0 requires prompt-toolkit<2.0.0,>=1.0.0, but you have prompt-toolkit 3.0.29 which is incompatible.\n",
      "google-colab 1.0.0 requires ipykernel~=4.10, but you have ipykernel 6.15.0 which is incompatible.\n",
      "google-colab 1.0.0 requires ipython~=5.5.0, but you have ipython 7.34.0 which is incompatible.\n",
      "google-colab 1.0.0 requires tornado~=5.1.0; python_version >= \"3.0\", but you have tornado 6.1 which is incompatible.\u001b[0m\n",
      "Successfully installed blinker-1.4 commonmark-0.9.1 gitdb-4.0.9 gitpython-3.1.27 ipykernel-6.15.0 ipython-7.34.0 jupyter-client-7.3.4 prompt-toolkit-3.0.29 pydeck-0.7.1 pympler-1.0.1 rich-12.4.4 smmap-5.0.0 streamlit-1.10.0 toml-0.10.2 tornado-6.1 validators-0.20.0 watchdog-2.1.9\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "pip_warning": {
        "packages": [
         "IPython",
         "prompt_toolkit",
         "tornado"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting pyspark\n",
      "  Downloading pyspark-3.3.0.tar.gz (281.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 281.3 MB 39 kB/s \n",
      "\u001b[?25hCollecting py4j==0.10.9.5\n",
      "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
      "\u001b[K     |████████████████████████████████| 199 kB 47.7 MB/s \n",
      "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pyspark: filename=pyspark-3.3.0-py2.py3-none-any.whl size=281764026 sha256=93cf7b7ccaa311ab24ce357fad3e2a6c9e237b9500cfb405c579c4942d4835ef\n",
      "  Stored in directory: /root/.cache/pip/wheels/7a/8e/1b/f73a52650d2e5f337708d9f6a1750d451a7349a867f928b885\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.5 pyspark-3.3.0\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting wget\n",
      "  Downloading wget-3.2.zip (10 kB)\n",
      "Building wheels for collected packages: wget\n",
      "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9675 sha256=07a51ed1aae489301428618481582e867a0ce98c8c82beeded95dcd5239746fe\n",
      "  Stored in directory: /root/.cache/pip/wheels/a1/b6/7c/0e63e34eb06634181c63adacca38b79ff8f35c37e3c13e3c02\n",
      "Successfully built wget\n",
      "Installing collected packages: wget\n",
      "Successfully installed wget-3.2\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting pyvis\n",
      "  Downloading pyvis-0.2.1.tar.gz (21 kB)\n",
      "Requirement already satisfied: jinja2>=2.9.6 in /usr/local/lib/python3.7/dist-packages (from pyvis) (2.11.3)\n",
      "Requirement already satisfied: networkx>=1.11 in /usr/local/lib/python3.7/dist-packages (from pyvis) (2.6.3)\n",
      "Requirement already satisfied: ipython>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from pyvis) (7.34.0)\n",
      "Collecting jsonpickle>=1.4.1\n",
      "  Downloading jsonpickle-2.2.0-py2.py3-none-any.whl (39 kB)\n",
      "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.7/dist-packages (from ipython>=5.3.0->pyvis) (0.18.1)\n",
      "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=5.3.0->pyvis) (2.6.1)\n",
      "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython>=5.3.0->pyvis) (57.4.0)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=5.3.0->pyvis) (4.4.2)\n",
      "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=5.3.0->pyvis) (0.7.5)\n",
      "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython>=5.3.0->pyvis) (5.1.1)\n",
      "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.7/dist-packages (from ipython>=5.3.0->pyvis) (0.1.3)\n",
      "Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython>=5.3.0->pyvis) (0.2.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.7/dist-packages (from ipython>=5.3.0->pyvis) (4.8.0)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from ipython>=5.3.0->pyvis) (3.0.29)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.16->ipython>=5.3.0->pyvis) (0.8.3)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2>=2.9.6->pyvis) (2.0.1)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from jsonpickle>=1.4.1->pyvis) (4.11.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect>4.3->ipython>=5.3.0->pyvis) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=5.3.0->pyvis) (0.2.5)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->jsonpickle>=1.4.1->pyvis) (4.1.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->jsonpickle>=1.4.1->pyvis) (3.8.0)\n",
      "Building wheels for collected packages: pyvis\n",
      "  Building wheel for pyvis (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pyvis: filename=pyvis-0.2.1-py3-none-any.whl size=23688 sha256=dd9000ee2e74c4b8c43f33ec1caa1c15dfccefdf8c99cd23283bcb13931cd775\n",
      "  Stored in directory: /root/.cache/pip/wheels/2a/8f/04/6340d46afc74f59cc857a594ca1a2a14a1f4cbd4fd6c2e9306\n",
      "Successfully built pyvis\n",
      "Installing collected packages: jsonpickle, pyvis\n",
      "Successfully installed jsonpickle-2.2.0 pyvis-0.2.1\n"
     ]
    }
   ],
   "source": [
    "!pip install streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9GmXzNUA0yrH",
    "outputId": "8ea5ba86-f1b0-418a-9207-34d981584cca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile app.py\n",
    "\n",
    "print('installing libraries')\n",
    "\n",
    "import streamlit as st\n",
    "import streamlit.components.v1 as components\n",
    "import pyspark\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark import SparkContext, SparkConf\n",
    "import requests\n",
    "import wget\n",
    "import csv\n",
    "import os\n",
    "from urllib import request\n",
    "from urllib.error import HTTPError\n",
    "from pyvis.network import Network\n",
    "\n",
    "print('all libraries are imported')\n",
    "\n",
    "def download_dataset(start_block, end_block, directory, debug=False):\n",
    "    if 'blocks-{}-{}-clustered'.format(start_block, end_block) in os.listdir(directory):\n",
    "        # dataset is already in directory\n",
    "        d_path = os.path.join(directory, 'blocks-{}-{}-clustered'.format(start_block, end_block))\n",
    "        v_path = os.path.join(d_path, 'vertices-{}-{}'.format(start_block, end_block))\n",
    "        e_path = os.path.join(d_path, 'edges-{}-{}'.format(start_block, end_block))\n",
    "        a_path = os.path.join(d_path, 'addresses-{}-{}'.format(start_block, end_block))\n",
    "        if debug: print('dataset is already in {}'.format(d_path))\n",
    "    else:\n",
    "        d_path = os.path.join(directory, 'blocks-{}-{}-clustered'.format(start_block, end_block))\n",
    "        os.mkdir(d_path)\n",
    "        try:\n",
    "            # try to dump dataset from github repo datasets\n",
    "            v_path = os.path.join(d_path, 'vertices-{}-{}'.format(start_block, end_block))\n",
    "            e_path = os.path.join(d_path, 'edges-{}-{}'.format(start_block, end_block))\n",
    "            a_path = os.path.join(d_path, 'addresses-{}-{}'.format(start_block, end_block))\n",
    "            wget.download('https://raw.github.com/VincenzoImp/Bitcoin-Address-Clustering/master/dataset/blocks-{}-{}-clustered/vertices-{}-{}.tar.gz'.format(start_block, end_block, start_block, end_block), out=d_path)\n",
    "            wget.download('https://raw.github.com/VincenzoImp/Bitcoin-Address-Clustering/master/dataset/blocks-{}-{}-clustered/edges-{}-{}.tar.gz'.format(start_block, end_block, start_block, end_block), out=d_path)\n",
    "            wget.download('https://raw.github.com/VincenzoImp/Bitcoin-Address-Clustering/master/dataset/blocks-{}-{}-clustered/addresses-{}-{}.tar.gz'.format(start_block, end_block, start_block, end_block), out=d_path)\n",
    "            os.system('tar -xf '+v_path.replace(' ', '\\ ')+'.tar.gz -C '+d_path.replace(' ', '\\ '))\n",
    "            os.system('tar -xf '+e_path.replace(' ', '\\ ')+'.tar.gz -C '+d_path.replace(' ', '\\ '))\n",
    "            os.system('tar -xf '+a_path.replace(' ', '\\ ')+'.tar.gz -C '+d_path.replace(' ', '\\ '))\n",
    "            os.remove(v_path+'.tar.gz')\n",
    "            os.remove(e_path+'.tar.gz')\n",
    "            os.remove(a_path+'.tar.gz')\n",
    "            if debug: print('dataset downloaded from https://github.com/VincenzoImp/Bitcoin-Address-Clustering/master/dataset/blocks-{}-{}-clustered'.format(start_block, end_block))\n",
    "        except HTTPError: \n",
    "            v_path, e_path, a_path, d_path = None, None, None, None\n",
    "    return v_path, e_path, a_path, d_path\n",
    "\n",
    "def generate_pyvis_graph(v_df, e_df, a_df, cluster_id_selected):\n",
    "    addr_clust = {row.address : row.cluster_id for row in a_df.rdd.collect()}\n",
    "    net = Network(\n",
    "                height='100%',\n",
    "                width='100%',\n",
    "                directed=True\n",
    "                )\n",
    "    net.repulsion(\n",
    "                node_distance=420,\n",
    "                central_gravity=0.33,\n",
    "                spring_length=110,\n",
    "                spring_strength=0.10,\n",
    "                damping=0.95\n",
    "                )\n",
    "    for node in v_df.rdd.collect():\n",
    "        if node.note == 'tx':\n",
    "            title = 'tx_hash: {}\\n'.format(node.tx_hash) + \\\n",
    "                'block_height: {}\\n'.format(node.block_height) + \\\n",
    "                'n_input: {}\\n'.format(node.n_input) + \\\n",
    "                'n_output: {}\\n'.format(node.n_output) + \\\n",
    "                'amount_input: {} BTC\\n'.format(node.amount_input/100000000) + \\\n",
    "                'amount_output: {} BTC\\n'.format(node.amount_output/100000000) + \\\n",
    "                'fee: {} BTC'.format(node.fee/100000000)\n",
    "            net.add_node(n_id=str(node.id), shape='dot', title=title)\n",
    "        elif node.note == 'UTXO':\n",
    "            title = '{}\\n'.format(node.note) + \\\n",
    "                'amount: {} BTC'.format(node.amount_input/100000000)\n",
    "            net.add_node(n_id=str(node.id), shape='square', title=title)\n",
    "        elif node.note == 'coinbase':\n",
    "            title = '{}\\n'.format(node.note) + \\\n",
    "                'amount: {} BTC'.format(node.amount_output/100000000)\n",
    "            net.add_node(n_id=str(node.id), shape='square', title=title)\n",
    "        elif node.note == 'unknown_tx':\n",
    "            title = 'unknown_tx\\n' + 'tx_index: {}'.format(node.id)\n",
    "            net.add_node(n_id=str(node.id), shape='square', title=title)\n",
    "    for edge in e_df.rdd.collect():\n",
    "        if edge.address[:4] != 'UTXO' and edge.address[:4] != 'coin':\n",
    "            title = 'address: {}\\n'.format(edge.address) + \\\n",
    "                    'value: {} BTC\\n'.format(edge.value/100000000) + \\\n",
    "                    'cluster_id : {}'.format(addr_clust[edge.address]) \n",
    "            if addr_clust[edge.address] == cluster_id_selected:\n",
    "                net.add_edge(source=str(edge.src_id), to=str(edge.dst_id), title=title, value=edge.value/100000000, color='red')\n",
    "            else:\n",
    "                net.add_edge(source=str(edge.src_id), to=str(edge.dst_id), title=title, value=edge.value/100000000)\n",
    "        else:\n",
    "            title = 'address: {}\\n'.format(edge.address) + \\\n",
    "                    'value: {} BTC\\n'.format(edge.value/100000000)\n",
    "            net.add_edge(source=str(edge.src_id), to=str(edge.dst_id), title=title, value=edge.value/100000000)\n",
    "    return net\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def main(start_block, end_block):\n",
    "    \n",
    "    print('executing main')\n",
    "\n",
    "    DIR = './'\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "    print('downloading dataset')\n",
    "\n",
    "    v_path, e_path, a_path, d_path = download_dataset(start_block, end_block, DIR, True)\n",
    "\n",
    "    print('loading dataset')\n",
    "\n",
    "    v_df = spark.read.load(v_path, \n",
    "                            format=\"csv\", \n",
    "                            sep=\",\", \n",
    "                            inferSchema=\"true\", \n",
    "                            header=\"true\"\n",
    "                            )\n",
    "    e_df = spark.read.load(e_path, \n",
    "                            format=\"csv\", \n",
    "                            sep=\",\", \n",
    "                            inferSchema=\"true\", \n",
    "                            header=\"true\"\n",
    "                            )\n",
    "    a_df = spark.read.load(a_path, \n",
    "                            format=\"csv\", \n",
    "                            sep=\",\", \n",
    "                            inferSchema=\"true\", \n",
    "                            header=\"true\"\n",
    "                            )\n",
    "    unknown_v_df = e_df.select('src_id').union(e_df.select('dst_id')).distinct().subtract(v_df.select('id')).withColumnRenamed('src_id', 'id')\n",
    "    unknown_v_df = unknown_v_df.withColumn('note', lit('unknown_tx')) \\\n",
    "                            .withColumn('tx_hash', lit(None)) \\\n",
    "                            .withColumn('block_height', lit(-1)) \\\n",
    "                            .withColumn('block_hash', lit(None)) \\\n",
    "                            .withColumn('fee', lit(0)) \\\n",
    "                            .withColumn('n_input', lit(0)) \\\n",
    "                            .withColumn('amount_input', lit(0)) \\\n",
    "                            .withColumn('n_output', lit(0)) \\\n",
    "                            .withColumn('amount_output', lit(0)) \\\n",
    "                            .withColumn('temporal_index', lit(-1))\n",
    "    v_df = v_df.union(unknown_v_df)\n",
    "\n",
    "    print('dataset loaded')\n",
    "\n",
    "    st.set_page_config(\n",
    "        page_title=\"Bitcoin Address Clustering\",\n",
    "        layout=\"wide\"\n",
    "    )\n",
    "    st.title('Bitcoin Address Clustering')\n",
    "    st.subheader('''\n",
    "            Enter a bitcoin address to be clustered using the following heuristics:\\n\n",
    "            – common-input-ownership heuristic\\n\n",
    "            – change address detection heuristic\\n\n",
    "            – Coinbase transaction mining address clustering heuristic\\n\n",
    "            – multiple mining pool address clustering heuristic\\n\n",
    "            – mixed transaction recognition heuristic\\n\n",
    "            – Louvain community detection algorithm\\n'''.format(start_block, end_block)\n",
    "            )\n",
    "    with st.container():\n",
    "        st1, st2 = st.columns(2)\n",
    "        with st1.form(key='form'):\n",
    "            address = st.text_input(label='Insert address to cluster')\n",
    "            submit_button = st.form_submit_button(label='Start Clustering')\n",
    "\n",
    "    if submit_button:\n",
    "        with st.spinner(\"Loading...\"):\n",
    "            \n",
    "            a_df.createOrReplaceTempView('ADDRESSES')\n",
    "            if spark.sql(\"select * from ADDRESSES where address = '\"+address+\"'\").rdd.isEmpty():\n",
    "                print('invalid address')\n",
    "                st.error('Invalid address')\n",
    "            else:\n",
    "                a_df.createOrReplaceTempView('ADDRESSES')\n",
    "                cluster_id = spark.sql(\"select cluster_id from ADDRESSES where address = '\"+address+\"'\").head().cluster_id\n",
    "                a_df.createOrReplaceTempView('ADDRESSES')\n",
    "                ClusterList = spark.sql(\"select address from ADDRESSES where cluster_id = \" + str(cluster_id)).withColumnRenamed('address', 'address_clustered')\n",
    "\n",
    "                print('address {} is in cluster {}'.format(address, cluster_id))\n",
    "                print('generating cluster graphs')\n",
    "\n",
    "                new_e_df = e_df.join(ClusterList, e_df.address == ClusterList.address_clustered, 'inner').drop('address_clustered')\n",
    "                tmp_df = new_e_df.select(new_e_df.src_id).union(new_e_df.select(new_e_df.dst_id)).withColumnRenamed('src_id', 'tmp_id').distinct()\n",
    "                new_v_df = v_df.join(tmp_df, v_df.id == tmp_df.tmp_id, 'inner').drop('tmp_id')\n",
    "                tmp_df = new_v_df.select('id')\n",
    "                new_e_df = e_df.join(tmp_df, tmp_df.id == e_df.src_id, 'inner').union(e_df.join(tmp_df, tmp_df.id == e_df.dst_id, 'inner')).drop('id').distinct()\n",
    "                tmp_df = new_e_df.select(new_e_df.src_id).union(new_e_df.select(new_e_df.dst_id)).distinct().withColumnRenamed('src_id', 'tmp_id')\n",
    "                new_v_df = v_df.join(tmp_df, v_df.id == tmp_df.tmp_id).drop('tmp_id')\n",
    "\n",
    "                new_pyvis_graph = generate_pyvis_graph(new_v_df, new_e_df, a_df, cluster_id)\n",
    "\n",
    "                new_pyvis_graph.height = '1200px'\n",
    "                new_pyvis_graph.width = '2400px'\n",
    "                new_pyvis_graph.write_html(os.path.join(d_path, 'cluster_graph-{}-{}.html'.format(start_block, end_block)))\n",
    "                \n",
    "                print('cluster graphs generated')\n",
    "                print('display cluster')\n",
    "                st.success(\"Done!\") \n",
    "                HtmlFile = open(os.path.join(d_path, 'cluster_graph-{}-{}.html'.format(start_block, end_block)), 'r', encoding='utf-8')\n",
    "                components.html(HtmlFile.read(), height=1200, scrolling=True)\n",
    "\n",
    "                print('cluster displayed')       \n",
    "\n",
    "                st.dataframe(new_v_df.toPandas())\n",
    "                st.dataframe(new_e_df.toPandas())\n",
    "                st.dataframe(ClusterList)\n",
    "\n",
    "    print('end main')\n",
    "    return\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    start_block = 0\n",
    "    end_block = 120000\n",
    "    main(start_block, end_block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OaTqnQaa0M72",
    "outputId": "313d86ee-91a1-4d8d-9772-e833b15b8879"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-26 22:37:05.851 INFO    numexpr.utils: NumExpr defaulting to 2 threads.\n",
      "\u001b[0m\n",
      "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.2:8501\u001b[0m\n",
      "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://35.237.141.166:8501\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[K\u001b[?25hnpx: installed 22 in 3.069s\n",
      "\u001b[34m  Stopping...\u001b[0m\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!streamlit run app.py & npx localtunnel --port 8501 -s bitcoinprivate"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "sKhQWqDpkDA_",
    "j9mB8waGlabj",
    "IOZd-zdGlgfZ",
    "CdPCjEcYlwX-",
    "hCZ6G78Qozu5",
    "1ekR20IG8B9R",
    "woABEepU7I3x",
    "t4-FiNF8MNF7",
    "w5cH-NJrTdUW",
    "sDosNsJOz_bt",
    "0J5Hk2iD0GYP",
    "dp0C2f-oBrsz",
    "glg6XYCN45P1",
    "2x4FyX7pp16j"
   ],
   "name": "Bitcoin Address Clustering",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
